<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="A Topic Modelling collective" />
  <title>Topic Models</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Topic Models</h1>
<p class="author">A Topic Modelling collective</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#definitions">Definitions</a></li>
<li><a href="#from-words-to-vectors">From words to vectors</a>
<ul>
<li><a href="#sec:bow">Bag-of-Words and n-grams</a></li>
<li><a href="#preprocessing">Preprocessing</a>
<ul>
<li><a href="#sec:sw">Stopwords</a></li>
<li><a href="#stemming">Stemming</a></li>
</ul></li>
<li><a href="#sec:count">The count matrix</a></li>
<li><a href="#sec:tfidf">Term Frequency - Inverse Document Frequency correction</a></li>
</ul></li>
<li><a href="#ch:spect">Linear algebra methods</a>
<ul>
<li><a href="#latent-semantic-analysis">Latent Semantic Analysis</a></li>
<li><a href="#sec:nmf">Non-Negative Matrix Factorization</a></li>
</ul></li>
<li><a href="#probabilistic-methods">Probabilistic methods</a>
<ul>
<li><a href="#sec:plsa">Probabilistic Latent Semantic Analysis</a></li>
<li><a href="#sec:lda">Latent Dirichlet Allocation</a></li>
<li><a href="#hierarchical-dirichlet-process">Hierarchical Dirichlet Process</a></li>
</ul></li>
<li><a href="#ch:net">Network-based methods</a>
<ul>
<li><a href="#sec:tm">Topic Mapping</a></li>
<li><a href="#sec:hsbm">Hierarchical Stochastic Block Model</a></li>
</ul></li>
<li><a href="#models-for-corpora-with-metadata">Models for corpora with metadata</a></li>
<li><a href="#compare-topic-models">Compare Topic Models</a>
<ul>
<li><a href="#measures">Measures</a>
<ul>
<li><a href="#normalized-mutual-information">Normalized Mutual Information</a></li>
</ul></li>
<li><a href="#example-corpora">Example corpora</a></li>
</ul></li>
</ul>
</nav>
<p>LICENSE CC-BY 4.0<br />
<img src="fig/cc-by.png" alt="image" /><br />
Source code available online at <a href="https://github.com/TopicModels/topicmodels.github.io">https://github.com/TopicModels/topicmodels.github.io</a></p>
<h1 id="definitions">Definitions</h1>
<p>We start by defining the object of our interest: a (textual) document.</p>
<div id="def:doc" class="definition">
<p><strong>Definition 1.1</strong> (Document).  <em>A document <span class="math inline">\(d\)</span> is an ordered set of <span class="math inline">\(\Omega_d\)</span> words with repetitions.<br />
We can formally write <span class="math inline">\(d = \{{\omega_d}_i\}_{i=1..\Omega_d}\)</span></em></p>
</div>
<p>This definition does not depend on the mathematical nature of words, which can be in fact actual words or even, for example, words and punctuation or DNA triplets. The only important thing is that the words belong to the same set.<br />
Similarly, we can define a corpus:</p>
<div class="definition">
<p><strong>Definition 1.2</strong> (Corpus).  <em>A corpus <span class="math inline">\(\Gamma\)</span> is a set of <span class="math inline">\(D\)</span> documents.<br />
We can formally write <span class="math inline">\(\Gamma = \{d_i\}_{i=1..D}\)</span></em></p>
</div>
<p>The next step is the important one: what is a Topic Model? Qualitatively, a Topic Model is a set of rules that classifies each document by the topic dealt with in it. Generally speaking, a model gives predictions with a certain degree of confidence. So, we expect that a Topic Model does not provide a unique matching between a document and a label, but, rather, that it provides a measure of the confidence for each matching, or equivalently a description of the mix of topics dealt with within each document. In other words, a Topic Model classifies the documents of a corpus in <span class="math inline">\(K\)</span> groups (called <strong>topics</strong>) by giving a score to each couple composed by a document <span class="math inline">\(d\)</span> and a topic <span class="math inline">\(k\)</span> which expresses the degree of confidence of the model in classifying the document <span class="math inline">\(d\)</span> in the topic <span class="math inline">\(k\)</span>. For clarity, we will call <span class="math inline">\(\mathcal{K}\)</span> the set of the <span class="math inline">\(K\)</span> topics.</p>
<div id="def:tm" class="definition">
<p><strong>Definition 1.3</strong> (Topic Model).  <em>A Topic Model is a function <span class="math inline">\(s: \Gamma \rightarrow {\mathbb{R}^+}^K\)</span> which maps each document of the corpus to a vector of scores, one for each topic. The score is increasing in the confidence of the model in classifying a document in each topic.</em></p>
</div>
<p>This definition is very general and somehow obscure, but it will become more clear when the algorithms to infer Topic Models will be introduced.</p>
<p>Another flaw of this definition is the difficulty in interpreting the values of the scores. What do the scores represent? How can we compare the scores among the documents of the corpus? We notice that if we normalize the score vector using a 1-norm (i.e. the sum of the values) we in fact project the vector to the K-1 simplex, which can be interpreted as a probability distribution, or as a distribution of topics inside documents.</p>
<div class="definition">
<p><strong>Definition 1.4</strong> (Normalized Topic Model).  <em>A normalized Topic Model is a function <span class="math inline">\(\tilde{s}: \Gamma \rightarrow {\Delta}^{K-1}\)</span> which gives the probability for each document to be classified in a topic. Additionally, we have <span class="math inline">\(\mathbb{P}(d|k) = \tilde{s}_k(d) = \frac{s_k(d)}{\|s(d)\|_1}\)</span> for <span class="math inline">\(k \in \mathcal{K}\)</span>.</em></p>
</div>
<p>Hereinafter, unless otherwise specified, when we will refer to a Topic Model we, in fact, will indicate a normalized Topic Model.</p>
<p>Finally, <span class="math inline">\(K\)</span> remains to be discusses. We have introduced the number of topics without specifying how to determine it. This is because we can divide Topic Model algorithms in two groups: <strong>parametric</strong> and <strong>non-parametric</strong> ones. A non-parametric algorithm is able to infer not only the model but also the number of topics, while a parametric one needs the number of topics to be fixed <em>a priori</em> to be fully defined, and it does not exist any general rule to fix <span class="math inline">\(K\)</span>.</p>
<h1 id="from-words-to-vectors">From words to vectors</h1>
<p>The definition of document <a href="#def:doc" data-reference-type="ref" data-reference="def:doc">Definition 1.1</a> is accurate and conveys all the available information, but it gives us a mathematical object (an ordered set) which is generally difficult to manage because it does not abstract (and so simplify) the problem, like a 1:1 map<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<p>The problems of such an accurate description are mainly two: computationally, we have no possibility to compress data, and so our analysis will be strongly limited by the computational resources we have available, thus preventing an application to large corpora; mathematically, we have too much information to be accounted for in the model, which strongly limits the formal tools we can use.</p>
<p>In this chapter we describe which hypotheses and methods are adopted in the context of the Topic Model algorithms, in order to get a more manageable representation of our data.</p>
<h2 id="sec:bow">Bag-of-Words and n-grams</h2>
<p>The goal of each Topic Model algorithm is to regroup documents which deal with the same topic. All the algorithms we will present in the following sections share some common hypotheses.</p>
<div id="hyp:topic" class="hypothesis">
<p><strong>Hypothesis 2.1</strong>.  <em>A topic can be effectively characterized by the words used to talk about it.</em></p>
</div>
<p>With this hypothesis we are assuming that when we switch topic in a text, the most significant change we make is the lexical one. Of course it will not be the only change we make, for example we can also change the average length of sentences, but this hypothesis states that this is the most important one.</p>
<div class="hypothesis">
<p><strong>Hypothesis 2.2</strong> (Bag-of-Words).  <em>We can describe a document as an <em>unordered</em> set of words with repetitions without losing too much information.</em></p>
</div>
<p>Given the hypothesis <a href="#hyp:topic" data-reference-type="ref" data-reference="hyp:topic">Hypothesis 2.1</a> it is natural to focus our attention more on the lexicon used in the documents than on the syntax. The Bag-of-Words hypothesis does exactly this: it states that the most important thing in a corpus to infer a Topic Model (and so the only one we have to care about in the algorithms) is the frequency of the words contained in the documents and not their order.</p>
<p>Sometimes this hypothesis is too strong, and we hypothesize that the order of the words is also locally (i.e. in a neighbourhood of each word in the ordered set) important, because the few words that precede or follow a word are important to specify the context, and so the meaning.</p>
<p>To recover the information brought by (local) order there are two possibilities: to abandon the Topic Model setting in favour of graphical models <span class="citation" data-cites="kang2018">(e.g. <a href="#ref-kang2018" role="doc-biblioref">Kang, Ahn, and Lee 2018</a>)</span>, however they are computationally very expensive, or to adopt the <em>n-grams</em> representation.</p>
<div class="definition">
<p><strong>Definition 2.1</strong> (N-grams representation).  <em>Given a document <span class="math inline">\(d\)</span> its n-grams representation is the ordered set with repetitions of the n-uple of consecutive words. We can express the n-grams representation <span class="math inline">\(\gamma_n(d) = \{\{\omega_i\}_{j..j+n-1}\}_{1..\Omega_d-n+1}\)</span>.</em></p>
</div>
<p>It is immediately evident that <span class="math inline">\(d = \gamma_1(d)\)</span> if we consider a 1-uple equivalent to the single element that composes it. Moreover we can extend the Bag-of-Words hypothesis to a new Bag-of-n-grams one, where the order is important inside each n-uple but not among them.</p>
<p>It is very common that when a n-gram representation is used, a range for n is considered: in practice both single words and 2-grams (bigrams) and eventually 3-grams (trigrams) are simultaneously considered as input for the algorithms. Formally we can express this idea as <span class="math inline">\(\gamma_{1..n}(d) = \cup_{i=1}^n \gamma_i(d)\)</span>.</p>
<h2 id="preprocessing">Preprocessing</h2>
<p>Preprocessing is the totality of the actions performed to transform the available sources of data, for example websites or paper documents, into a format which can be used as input for a Topic Model algorithm.</p>
<p>In this section we will not discuss the technical preprocessing required to transform data sources in a machine-readable set of words; instead, it is common to adopt some techniques to clean the data before passing them to an algorithm. There are a practical and a theoretical reason to do this: removing some uninteresting words from a document reduces the memory requirements to process the data, allowing for processing larger corpora; given the hypothesis <a href="#hyp:topic" data-reference-type="ref" data-reference="hyp:topic">Hypothesis 2.1</a> we can assume that certain words are not indicative of any topic (like pronouns, to be and to have verbs, conjunctions, etc.) or that they are not statistically significant because they are too much or too little frequent.<br />
We will define some transformations which generally redefine the set of words <span class="math inline">\(\Omega = \{{\omega_d}_i  ; \forall d,i\}\)</span> and transform a document by mapping a subset of words from the original set <span class="math inline">\(\Omega\)</span> to a new set <span class="math inline">\(\mathcal{W}\)</span>.</p>
<h3 id="sec:sw">Stopwords</h3>
<p>To reduce the dimensionality of the dataset and, hopefully, the noise in the data, we can omit some uninformative words from the documents, called stopwords. Which actually is the best way to choose those words is debated <span class="citation" data-cites="schofield2017">(<a href="#ref-schofield2017" role="doc-biblioref">Schofield, Magnusson, and Mimno 2017</a>)</span>.</p>
<p>Some possibilities are: compiling a list of words that are very common in the language (articles, pronouns, conjunctions, ...); filtering out the words with a very low or very high frequency or count<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, because those cannot be statistically significant (either for their scarcity or uniformity); using more complex schemes, based for example on information theory, to identify uninformative words <span class="citation" data-cites="gerlach2019">(<a href="#ref-gerlach2019" role="doc-biblioref">Gerlach, Shi, and Amaral 2019</a>)</span>.</p>
<p>Independently of how the list of stopwords <span class="math inline">\(S\)</span> is compiled, we can transform a document <span class="math inline">\(d\)</span> as <span class="math inline">\(d&#39; = \{\omega | \omega \in d \wedge \omega \notin S \}\)</span>.</p>
<h3 id="stemming">Stemming</h3>
<p>In our setting, where the lexicon is the most informative feature of a text, we can hypothesize that the singular and the plural form of a noun or different verb tenses can be treated as equivalent. The process of transforming all the inflected forms to a common base form is known as <em>stemming</em> and is generally performed by applying a list of rules to each word <span class="citation" data-cites="porter1980">(<a href="#ref-porter1980" role="doc-biblioref">Porter 1980</a>)</span> or by searching in dedicated databases <span class="citation" data-cites="miller1995">(<a href="#ref-miller1995" role="doc-biblioref">Miller 1995</a>)</span>. Formally we define a function <span class="math inline">\(\sigma: \Omega \rightarrow \mathcal{W}\)</span> which maps each word in our corpus to its base form. We expect that <span class="math inline">\(|\Omega| \geq |\mathcal{W}|\)</span>, where equivalence represents the meaningless cases in which <span class="math inline">\(\sigma\)</span> is a bijection. Stemming is useful to reduce the dimensionality of the data, but its effectiveness in improving the inferred model is debated <span class="citation" data-cites="schofield2016">(<a href="#ref-schofield2016" role="doc-biblioref">Schofield and Mimno 2016</a>)</span>.</p>
<h2 id="sec:count">The count matrix</h2>
<p>At the end of this section we will be able to abandon the set description in favour of a more manageable vectorial one.</p>
<p>Given a corpus <span class="math inline">\(\Gamma\)</span> we can preprocess it, eventually using the identity for <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(S = \emptyset\)</span> to leave it unchanged, obtaining <span class="math inline">\(\Gamma&#39; = \{d&#39;_i = \{w = \sigma(\omega)\, |\, \omega \in d_i \wedge \omega \notin S\} \; \forall d_i \in \Gamma \}\)</span>.</p>
<p>Since the original corpus <span class="math inline">\(\Gamma\)</span> and set of untransformed words <span class="math inline">\(\Omega\)</span> will not return in the next sections, we will refer to the transformed words <span class="math inline">\(w\)</span> as words and will indicate the transformed corpus as <span class="math inline">\(\Gamma\)</span>.</p>
<p>We can define a matrix <span class="math inline">\(C \in \mathbb{N}^{D \times W}\)</span>, where <span class="math inline">\(W=|\mathcal{W}|\)</span>, which represents the count of each word <span class="math inline">\(w\)</span> in each document <span class="math inline">\(d\)</span> and which is called count matrix. We have <span class="math inline">\(C_{dw} = |\{w_i | w_i = w \; \forall w_i \in d\}|\)</span><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<p>Sometimes we are interested in frequencies instead of counts, but it is straightforward to define a frequency matrix <span class="math inline">\(F \in {\mathbb{R}^+}^{D \times W}\)</span> which is obtained by row normalizing <span class="math inline">\(C\)</span> with the 1-norm.</p>
<h2 id="sec:tfidf">Term Frequency - Inverse Document Frequency correction</h2>
<p>Some methods we will describe in next chapters (particularly chapter <a href="#ch:spect" data-reference-type="ref" data-reference="ch:spect">3</a>) accept as input a generic <span class="math inline">\(D \times W\)</span> matrix, which leads to the question if there are better choices than count or frequency matrices. An alternative weight scheme known as Term Frequency - Inverse Document Frequency (TFIDF) <span class="citation" data-cites="salton1988">(<a href="#ref-salton1988" role="doc-biblioref">Salton and Buckley 1988</a>)</span>, tries to weight not only the frequency in each document but also the frequency among documents, in order to distinguish the words that are typical of some documents, and so can be typical of a topic, from those which are homogeneously distributed among documents, and so are less informative.</p>
<p>The proposed weight scheme leads to the matrix <span class="math inline">\(T_{dw} = \frac{C_{dw}}{\sum_w C_{dw}} \log \frac{D}{|\{d | w \in d\}|}\)</span> where the first factor is indeed <span class="math inline">\(F_{dw}\)</span> and the second is (a monotonic transformation of) the inverse of the fraction of documents in which the word appears. Sometimes the logarithm is substituted by the identity or its argument is replaced by <span class="math inline">\(\frac{D}{|\{d | w \in d\}| + 1}\)</span>, to have <span class="math inline">\(T_{dw} &gt; 0 \iff C_{dw} &gt; 0\)</span>.</p>
<h1 id="ch:spect">Linear algebra methods</h1>
<p>We can observe that the count-matrix (as well as frequency matrix and TFIDF matrix) can be seen as a list of row vectors which represent documents as a mixture of words, i.e. as a vector in the space spanned by the words used as canonical basis.</p>
<p>We can then reformulate the problem of finding a Topic Model as the research of a method which transforms this vector space to highlight similarities among documents.</p>
<p>The two methods we present in the next sections describe a document as a mixture of topics, each described as a mixture of words. So, we have to find a new basis, spanned by vectors in the space of words (which are the topics inferred by the model), which represent the document with sufficient accuracy. The new basis is defined by the original basis (the one spanned by the words) and a change-of-basis matrix, which in our case describes each topic of the new basis as a mixture of words.</p>
<p>Another possibility, which we will not discuss in detail, is to use non-linear methods to reduce the dimensionality of the words-spanned basis, like an embedding algorithm as Word2Vec <span class="citation" data-cites="mikolov2013">(<a href="#ref-mikolov2013" role="doc-biblioref">Mikolov et al. 2013</a>)</span> and/or a general-purpose dimensionality reduction algorithm as UMAP <span class="citation" data-cites="mcinnes2020">(<a href="#ref-mcinnes2020" role="doc-biblioref">McInnes, Healy, and Melville 2020</a>)</span>, and then using a soft-clustering algorithm which accepts vectors as input, like HDBSCAN <span class="citation" data-cites="campello2013">(<a href="#ref-campello2013" role="doc-biblioref">Campello, Moulavi, and Sander 2013</a>)</span>, to assign each document to a cluster (a topic) <span class="citation" data-cites="angelov2020 grootendorst2021">(<a href="#ref-angelov2020" role="doc-biblioref">Angelov 2020</a>; <a href="#ref-grootendorst2021" role="doc-biblioref">Grootendorst and Reimers 2021</a>)</span>.</p>
<p>We will see that the method presented in section <a href="#sec:nmf" data-reference-type="ref" data-reference="sec:nmf">3.2</a> can be interpreted as a particular case of this process, in which no dimensionality reduction is performed and no non-linear transformation is used.</p>
<p>In the next sections, we will collectively refer to the count matrix and its transformations as <span class="math inline">\(X\)</span>.</p>
<h2 id="latent-semantic-analysis">Latent Semantic Analysis</h2>
<p>Latent Semantic Analysis (LSA)<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> <span class="citation" data-cites="deerwester1990">(<a href="#ref-deerwester1990" role="doc-biblioref">Deerwester et al. 1990</a>)</span> decomposes the matrix <span class="math inline">\(X\)</span> into the product of three matrices <span class="math inline">\(X\approx U_K \Sigma_K V_K\)</span>, such that <span class="math inline">\(\Sigma_K \in {\mathbb{R}^+}^{K\times K}\)</span> and diagonal, <span class="math inline">\(U_K \in \mathbb{R}^{D\times K}\)</span> and <span class="math inline">\(U_K^T U_K=\mathbb{I}_K\)</span> and <span class="math inline">\(V_K \in \mathbb{R}^{K\times W}\)</span> and <span class="math inline">\(V_K V_K^T=\mathbb{I}_K\)</span> (orthogonal).<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<p>To determine these matrices we start from a simpler case in which <span class="math inline">\(K=\min(D,W)\)</span>.</p>
<p>We observe that <span class="math inline">\(XX^T \in \mathbb{R}^{D\times D}\)</span> and <span class="math inline">\(X^T X \in \mathbb{R}^{W\times W}\)</span> are two symmetric matrices with values in <span class="math inline">\(\mathbb{R}^+\)</span> and so, by the spectral theorem, they can be represented as diagonal matrices in a specific basis identified by an orthogonal (change-of-basis) matrix. Formally, <span class="math inline">\(XX^T = U&#39; \Gamma U&#39;^T\)</span>, where <span class="math inline">\(U&#39;\)</span> is orthogonal and <span class="math inline">\(\Gamma\)</span> is diagonal, both with shape <span class="math inline">\(D \times D\)</span>, and <span class="math inline">\(X^T X = V&#39;^T \Lambda V&#39;\)</span>, where <span class="math inline">\(V&#39;\)</span> is orthogonal and <span class="math inline">\(\Lambda\)</span> is diagonal, both with shape <span class="math inline">\(W\times W\)</span>. All the elements of <span class="math inline">\(\Lambda\)</span> and <span class="math inline">\(\Gamma\)</span> are non negative, since they are 0 or the eigenvalues of <span class="math inline">\(XX^T\)</span> or <span class="math inline">\(X^TX\)</span> which are non negative<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>.</p>
<p>Since <span class="math inline">\(XX^T = (X^TX)^T\)</span>, <span class="math inline">\(\text{rank}(XX^T) = \text{rank}(X^TX) \leq K\)</span>, and so both <span class="math inline">\(\Gamma\)</span> and <span class="math inline">\(\Lambda\)</span> have at most <span class="math inline">\(K\)</span> non null values on the diagonal. So, given that the eigenvalues of a matrix and of its transpose are the same, it is possible to define a <span class="math inline">\(K\times K\)</span> diagonal matrix <span class="math inline">\(\Delta\)</span> whose non-zero diagonal elements are the non-null eigenvalues of <span class="math inline">\(\Gamma\)</span> and <span class="math inline">\(\Lambda\)</span>. The equalities <span class="math inline">\(XX^T = U \Delta U^T\)</span> and <span class="math inline">\(X^TX=V^T \Delta V\)</span> hold for the matrices <span class="math inline">\(U \in \mathbb{R}^{D \times K}\)</span> and <span class="math inline">\(V \in \mathbb{R}^{K \times W}\)</span> obtained by removing from <span class="math inline">\(U&#39;\)</span> <span class="math inline">\(D-K\)</span> columns and from <span class="math inline">\(V&#39;\)</span> <span class="math inline">\(W-K\)</span> rows corresponding to the eigenvectors with eigenvalue 0. If <span class="math inline">\(\text{rank}(XX^T) = K\)</span> <span class="math inline">\(\Delta\)</span> is uniquely defined, except for the order of the values on the diagonal.</p>
<p>Now, we can define a diagonal matrix <span class="math inline">\(\Sigma\)</span> whose elements are the positive<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> square roots of <span class="math inline">\(\Delta\)</span>. By orthogonality, it follows that <span class="math inline">\(XX^T = U \Sigma^2 U^T = U\Sigma V V^T \Sigma U^T\)</span> and <span class="math inline">\(X^T X = V^T \Sigma U^T U \Sigma V = V^T \Sigma^2 V\)</span> and, finally, <span class="math inline">\(X = U \Sigma V\)</span>.</p>
<p>If we decreasingly order the eigenvalues on the diagonal of <span class="math inline">\(\Sigma\)</span>, it becomes uniquely definite if the multiplicity of each eigenvalue is 1, i.e. each eigenvalue appears only one time.</p>
<p>We obtain a decomposition like the one we are looking for, for the case <span class="math inline">\(K=\min(D,W)\)</span>. To extend this to a generic <span class="math inline">\(K\)</span>, we recall a known result <span class="citation" data-cites="eckart1936">(<a href="#ref-eckart1936" role="doc-biblioref">Eckart and Young 1936</a>)</span> which states that the best <span class="math inline">\(K\)</span>-rank approximation, in terms of the Frobenius norm<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>, of a matrix is its projection on the subspace spanned by the eigenvectors of the <span class="math inline">\(K\)</span> bigger eigenvalues in absolute value.</p>
<p>In our case it simplifies to keep the first <span class="math inline">\(K\)</span> columns of <span class="math inline">\(U\)</span>, to obtain the matrix <span class="math inline">\(U_K\)</span>, the first <span class="math inline">\(K\)</span> values on <span class="math inline">\(\Sigma\)</span> diagonal, to obtain <span class="math inline">\(\Sigma_K\)</span>, and the first <span class="math inline">\(K\)</span> rows of <span class="math inline">\(V\)</span>, to obtain <span class="math inline">\(V_K\)</span>.</p>
<p>It can appear easy to interpret this decomposition as a Topic Model: <span class="math inline">\(U\)</span> expresses each document as a mixture of topics, <span class="math inline">\(V\)</span> each topic as a mixture of words and <span class="math inline">\(\Sigma\)</span> the prevalence of each topic in the corpus.</p>
<p>An advantage of this interpretation is that the similarity between two documents (or topics) can be measured by their cosine similarity<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>.</p>
<p>Following this interpretation, the scores of each document with regard to the topics are the row of <span class="math inline">\(U\)</span> which are vectors defined in <span class="math inline">\(\mathbb{R}^K\)</span>, while in the definition <a href="#def:tm" data-reference-type="ref" data-reference="def:tm">Definition 1.3</a> we have required them to be defined in <span class="math inline">\({\mathbb{R}^+}^K\)</span> and there is no meaningful way to transform <span class="math inline">\(U\)</span> to have only non-negative values. Moreover, the requirement that the score is increasing in the confidence of the classification is not clearly met.</p>
<p>To overcome this limitation the next section introduces a different matrix decomposition which satisfies our requirements for a Topic Model (even though with its own flaws).</p>
<h2 id="sec:nmf">Non-Negative Matrix Factorization</h2>
<p>The information we use from the SVD decomposition is conveyed from the matrices <span class="math inline">\(U_K\)</span> and <span class="math inline">\(V_K\)</span>.</p>
<p>So, to simplify the problem and overcome the limitations of the SVD decomposition, we look for a decomposition in the form <span class="math inline">\(X \approx U_K V_K\)</span><a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>, where <span class="math inline">\(U_K \in {\mathbb{R}^+}^{D\times K}\)</span> and <span class="math inline">\(V_K \in {\mathbb{R}^+}^{K \times W}\)</span>. We enforce the non negativity of the elements of the matrices in order to be able to interpret the matrix <span class="math inline">\(U_K\)</span> as a score, as defined in <a href="#def:tm" data-reference-type="ref" data-reference="def:tm">Definition 1.3</a><a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>. Such decomposition is known as Non-Negative Matrix Factorization (NMF).</p>
<p>While it is possible to define the SVD decomposition in a way such that it is unique, this is not possible with NMF, and the decomposition obtained depends on the choice of the error function to be minimized and the initialization of the algorithm used <span class="citation" data-cites="langville2014 berry2007 lee2000">(<a href="#ref-langville2014" role="doc-biblioref">Langville et al. 2014</a>; <a href="#ref-berry2007" role="doc-biblioref">Berry et al. 2007</a>; <a href="#ref-lee2000" role="doc-biblioref">Lee and Seung 2000</a>)</span>. Two typical choices for the error function are the Frobenius squared-norm <span class="math inline">\(\| X - UV \|_F^2 = \sum (X_{ij} - (UV)_{ij})^2\)</span> and the (generalized) Kullback-Leiber divergence <span class="math inline">\(D(X\|UV) = \sum(X_{ij}\log\frac{X_{ij}}{(UV)_{ij}}-X_{ij}+(UV)_{ij})\)</span><a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> <span class="citation" data-cites="lee2000">(<a href="#ref-lee2000" role="doc-biblioref">Lee and Seung 2000</a>)</span> while the possible choices of the initial guesses for <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are many <span class="citation" data-cites="langville2014">(<a href="#ref-langville2014" role="doc-biblioref">Langville et al. 2014</a>)</span>.</p>
<p>The algorithms proposed rely on the fact that while it is not possible to find a global minimum for <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> at the same time, it is possible to minimize one of the two matrices keeping the other fixed. So, given the initial guessing, the matrices are alternatively updated trying to minimize the error function: two common algorithms are the multiplicative update <span class="citation" data-cites="lee2000">(<a href="#ref-lee2000" role="doc-biblioref">Lee and Seung 2000</a>)</span> and the coordinate descending <span class="citation" data-cites="hsieh2011">(<a href="#ref-hsieh2011" role="doc-biblioref">Hsieh and Dhillon 2011</a>)</span>.</p>
<p>If we use <span class="math inline">\(X=F\)</span>, the frequency matrix, we can interpret the entries of the matrix as the probability (i.e. frequency) of a word in a given document <span class="math inline">\(\mathbb{P}(w|d)\)</span>. It is then straightforward trying to interpret <span class="math inline">\(X \approx U_K V_K\)</span> as <span class="math inline">\(\mathbb{P}(w|d) \approx \sum_k \mathbb{P}(k|d) \mathbb{P}(w|k)\)</span>. To do so, we can enforce the row normalization of <span class="math inline">\(U_K\)</span> and <span class="math inline">\(V_K\)</span>.</p>
<p>This interpretation leads to the probabilistic models presented in the next chapter, and particularly Probabilistic Latent Semantic Analysis (PLSA) presented in section <a href="#sec:plsa" data-reference-type="ref" data-reference="sec:plsa">4.1</a>.</p>
<p>Finally, <span class="citation" data-cites="kim2008">Kim and Park (<a href="#ref-kim2008" role="doc-biblioref">2008</a>)</span> have discussed the additional hypothesis under which NMF is equivalent to a known cluster algorithm (k-means), as stated in the introduction of this chapter, while <span class="citation" data-cites="ding2005">Ding, He, and Simon (<a href="#ref-ding2005" role="doc-biblioref">2005</a>)</span> have demonstrated that “<em>[Decomposing a matrix as <span class="math inline">\(X = U_K V_K\)</span>] is equivalent to simultaneous clustering of rows and columns of a bipartite graph</em>”. This last result can appear now out of context, but it foretells the intuition behind the methods that will be discussed in chapter <a href="#ch:net" data-reference-type="ref" data-reference="ch:net">5</a> (and particularly in section <a href="#sec:hsbm" data-reference-type="ref" data-reference="sec:hsbm">5.2</a>).</p>
<h1 id="probabilistic-methods">Probabilistic methods</h1>
<p>Another possibility to tackle the problem of developing a Topic Model algorithm is to describe a document as the result of a stochastic process.</p>
<p>For each document <span class="math inline">\(d\)</span> of length <span class="math inline">\(n_d\)</span> (which can be observed or be the realization of a random variable, generally a Poisson one), we have to extract <span class="math inline">\(n_d\)</span> words. This is generally modelled as the subsequent extraction for each position of a topic, often from a document-specific distribution, and then a word from a topic-specific distribution.</p>
<p>All the models presented in this section model this stochastic process using different probability distributions and or different ways to determine the number of topics present in the corpus, but other strategies still based on a probabilistic reasoning are possible (see infra section <a href="#sec:hsbm" data-reference-type="ref" data-reference="sec:hsbm">5.2</a> or <span class="citation" data-cites="arora2012">(<a href="#ref-arora2012" role="doc-biblioref">Arora et al. 2012</a>)</span>).</p>
<p>Finally, this approach allows to generate synthetic corpora by specifying all the parameters the model is designed to infer from data and iterating the stochastic process for a given set of (symbolic) words (for example the numbers from 1 to <span class="math inline">\(W\)</span>). An application of synthetic corpora will be discussed in part <a href="#part:ii" data-reference-type="ref" data-reference="part:ii">[part:ii]</a>, exploring the strategies to compare and benchmark different Topic Models.</p>
<h2 id="sec:plsa">Probabilistic Latent Semantic Analysis</h2>
<p>The simplest possible model is the one which models the choice of each word in a document as <span class="math inline">\(\mathbb{P}(w|d) = \sum_k \mathbb{P}(k|d) \mathbb{P}(w|k)\)</span>, where both <span class="math inline">\(\mathbb{P}(w|k)\)</span> and <span class="math inline">\(\mathbb{P}(k|d)\)</span> are categorical distributions without additional constraints. This model is known as Probabilistic Latent Semantic Analysis (PLSA)<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> <span class="citation" data-cites="hofmann1999">(<a href="#ref-hofmann1999" role="doc-biblioref">Hofmann 1999</a>)</span>.</p>
<p>This is a parametric model, and one of the biggest limits of this method is the very high number of free parameters to be determined (<span class="math inline">\(D\times (K-1) + K \times (W-1)\)</span>), which easily leads to over-fitting the model. Given the probabilistic formulation of the model the study of the likelihood of the corpus (or of a subset of it) as function of <span class="math inline">\(K\)</span> can provide a method of choice, but it is useful to remember that the likelihood tends to increase with the number of parameters, distorting the comparison<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>.</p>
<p>Before discussing how to infer the model, we have to highlight the similarity between this model and the interpretation we have given of NMF in section <a href="#sec:nmf" data-reference-type="ref" data-reference="sec:nmf">3.2</a>. In fact <span class="citation" data-cites="gaussier2005">Gaussier and Goutte (<a href="#ref-gaussier2005" role="doc-biblioref">2005</a>)</span> have demonstrated that under certain conditions PLSA and NMF infer the same Topic Model. This explains also the name: PLSA was created as a variation of LSA with a probabilistic interpretation, exactly as we have introduced NMF.</p>
<p>PLSA infers a Topic Model using an Expectation-Maximization (EM) algorithm, in which an E-step (in which we assume to know the topic from which each word is drawn) and a M-step(in which we look for the best-fitting topic for each word) are iterated alternatively. Even for PLSA it is necessary to provide guesses for <span class="math inline">\(\mathbb{P}(w|k)\)</span> and <span class="math inline">\(\mathbb{P}(k|d)\)</span>, like in NMF, and the same techniques of initialization can be used <span class="citation" data-cites="langville2014">(see for example <a href="#ref-langville2014" role="doc-biblioref">Langville et al. 2014</a>)</span>.</p>
<p>If we define <span class="math inline">\(\phi_{kw} = \mathbb{P}(w|k)\)</span>, <span class="math inline">\(\theta_{dk} = \mathbb{P}(k|d)\)</span> and <span class="math inline">\(n_{dw}\)</span> the entries of the count matrix <span class="math inline">\(C\)</span>, we can write <span class="math inline">\(\mathbb{P}(d) = \prod_{w\in \mathcal{W}} (\sum_{k \in \mathcal{K}}\theta_{dk}\phi_{kw})^{n_{dw}}\)</span>.</p>
<p>The likelihood of the data (the corpus) will be <span class="math display">\[\mathcal{L} = \mathbb{P}(\Gamma) = \prod_{d \in \Gamma}\prod_{w\in \mathcal{W}} (\sum_{k \in \mathcal{K}}\theta_{dk}\phi_{kw})^{n_{dw}}\]</span> from which we can formulate the problem as <span class="math display">\[\mathop{\mathrm{arg\,max}}_{\theta \phi} \Lambda = \mathop{\mathrm{arg\,max}}_{\theta \phi} \left[\log\mathcal{L} + \sum_d \lambda_p(1-\sum_k \theta_{dk}) + \sum_k \mu_k (1-\sum_w \phi_{kw})\right]\]</span> where we have incorporated the requirement for <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\phi\)</span> to be categorical distributions fixed respectively <span class="math inline">\(d\)</span> and <span class="math inline">\(k\)</span>.</p>
<p>For the E-step, for which we assume to know the topic from which each word is chosen (which is our latent, i.e. unobservable, variable), we recall that <span class="math inline">\(w_{di} \in d\)</span> represents each word in each different position in a document and we introduce <span class="math inline">\(R_{dik}\)</span> which is 1 if the word <span class="math inline">\(w_{di}\)</span> is chosen from the topic <span class="math inline">\(k\)</span> and 0 otherwise. We can rewrite the likelihood as <span class="math inline">\(\mathcal{L} = \prod_d \prod_i \prod_k (\theta_{dk}\phi_{kw})^{R_{dik}}\)</span> and consequently <span class="math inline">\(\log\mathcal{L} = \sum_d \sum_i \sum_k R_{dik} (\log\theta_{dk} + \log\phi_{kw})\)</span>.</p>
<p>Since we do not actually know which is the right topic we can approximate <span class="math inline">\(R\)</span> by its expectation <span class="math inline">\(\mathbb{E}(R_{dik}) = \mathbb{P}(R_{dik} = 1 | \Gamma, \theta, \phi) = \frac{\mathbb{P}(R_{dik} = 1, \Gamma | \theta, \phi)}{\mathbb{P}(\Gamma | \theta, \phi)} = \frac{\mathbb{P}(R_{dik} = 1, w_{di} | \theta_{dk}, \phi_{kw})}{\mathbb{P}(w_{di} | \theta_{dk}, \phi_{kw})} = \frac{\theta_{dk} \phi_{kw}}{\sum_k \theta_{dk} \phi_{kw}}\)</span>.</p>
<p>Then we use this result in the M-step in which we look for <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\phi\)</span> to maximize <span class="math inline">\(\Lambda\)</span>.</p>
<p>Using <span class="math inline">\(\mathbb{1}(w_{di} = w))\)</span> to denote the fact that for each position we can observe the realization of the stochastic process (the word in the text), we have <span class="math inline">\(\Lambda = \sum_{dik} \left[R_{dik} (\log \theta_{dk} + \log \phi_{kw} \mathbb{1}(w_{di} = w))\right] + \sum_d \lambda_p(1-\sum_k \theta_{dk}) + \sum_k \mu_k (1-\sum_w \phi_{kw})\)</span> and solving the optimization problem we find <span class="math inline">\(\theta_{dk} \propto {\sum_i R_{dik}}\)</span> and <span class="math inline">\(\phi_{kw} \propto \sum_{di} R_{dik} \mathbb{1}(w_{di} = w)\)</span>.</p>
<p>Finally, using the expectation of <span class="math inline">\(R\)</span> to approximate it, and observing that <span class="math inline">\(\mathbb{E}(R)\)</span> does not depend on <span class="math inline">\(i\)</span> we find <span class="math display">\[\begin{aligned}
        \theta_{dk} \propto {\sum_w n_{dw} R_{dwk}} \qquad \text{s.t. $\sum_k \theta_{dk} = 1$}\\
        \phi_{kw} \propto \sum_{d} n_{dw} R_{dwk} \qquad \text{s.t. $\sum_w \phi_{kw} = 1$}\\
        R_{dwk} \propto \theta_{dk} \phi_{kw} \qquad \text{s.t. $\sum_k R_{dkw} = 1$}      \end{aligned}\]</span> with which we can update alternatively <span class="math inline">\(R\)</span> in the E-step and <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\phi\)</span> in the M-step until the value of <span class="math inline">\(\mathcal{L}\)</span> converges (or after an a-priori fixed number of iterations).</p>
<h2 id="sec:lda">Latent Dirichlet Allocation</h2>
<p>As stated above, a significant problem of PLSA is the very high number of unconstrained parameters in the model. An intuitive way to improve the algorithms is to put some constrains on the parameters.</p>
<p>The Latent Dirichlet Allocation (LDA) <span class="citation" data-cites="blei2003">(<a href="#ref-blei2003" role="doc-biblioref">Blei, Ng, and Jordan 2003</a>)</span> hypothesizes that the parameters of the categorical distributions for words and topics are drawn from two Dirichlet distributions, following a common practice in Bayesian statistics to couple a distribution with its conjugate prior<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a>. This procedure introduces constraints on the parameters, helping the inference of a model, but, at the same time, reducing which empirical distributions the model can fit.</p>
<figure>
<img src="fig/Smoothed_LDA.png" id="fig:smoothedlda" alt="This is the plate representation of LDA. \alpha and \beta are the parameters of the two Dirichlet distributions from which are drawn, respectively, the parameters of a document-specific categorical distribution of topics and a topic-specific categorical distribution of words. Given these two distribution the process is analogue to PLSA. By Slxu.public - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=7922733" /><figcaption aria-hidden="true">This is the plate representation of LDA. <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are the parameters of the two Dirichlet distributions from which are drawn, respectively, the parameters of a document-specific categorical distribution of topics and a topic-specific categorical distribution of words. Given these two distribution the process is analogue to PLSA.<br />
By Slxu.public - Own work, CC BY-SA 3.0, <a href="https://commons.wikimedia.org/w/index.php?curid=7922733">https://commons.wikimedia.org/w/index.php?curid=7922733</a></figcaption>
</figure>
<p>We call <span class="math inline">\(\alpha\)</span> the <span class="math inline">\(K\)</span>-vector of the parameters of the Dirichlet distribution from which are drawn the parameters of a document-specific categorical distribution of topics <span class="math inline">\(\theta_{dk}\)</span> and <span class="math inline">\(\beta\)</span> the <span class="math inline">\(W\)</span>-vector of the parameters of the Dirichlet distribution from which are drawn the parameters of a topic-specific categorical distribution of words <span class="math inline">\(\phi_{kw}\)</span>. LDA can be symmetric with respect to <span class="math inline">\(\alpha\)</span> or <span class="math inline">\(\beta\)</span> if all the elements of the vector have the same value or asymmetric if the vector is allowed to have different values for each element. <span class="citation" data-cites="wallach2009">Wallach, Mimno, and McCallum (<a href="#ref-wallach2009" role="doc-biblioref">2009</a>)</span> have argued in favour of having asymmetric <span class="math inline">\(\alpha\)</span> and symmetric <span class="math inline">\(\beta\)</span>, while <span class="citation" data-cites="shiNewEvaluationFramework2019">Shi et al. (<a href="#ref-shiNewEvaluationFramework2019" role="doc-biblioref">2019</a>)</span> have shown the effects of the choice of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> on the sparseness of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\phi\)</span>.</p>
<p>LDA is a parametric algorithm, and the same considerations made for PLSA on how to determine <span class="math inline">\(K\)</span> still apply.</p>
<p>To infer an LDA model there are two possibilities. The first is to use the EM-like algorithm (also called Variation Bayes - VB) introduced by <span class="citation" data-cites="blei2003">Blei, Ng, and Jordan (<a href="#ref-blei2003" role="doc-biblioref">2003</a>)</span> which operates similarly to the one discussed for PLSA, where the latent variables introduced split the problem in two independent parts: find the topics of each document, and generate the words in the document from the chosen topics. The second is to use the MonteCarlo - Markov Chain (MCMC) algorithm introduced by <span class="citation" data-cites="griffiths2004">Griffiths and Steyvers (<a href="#ref-griffiths2004" role="doc-biblioref">2004</a>)</span> (based on the Gibbs’ sampling scheme - GS) in which the estimations of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\phi\)</span> are progressively improved by iterating over each word in the corpus and assigning this word to a topic with a probability estimated on the words previously seen in the corpus. For both these algorithms a parallel version has been developed and for the variational one also an online one (i.e. one document at time) is available <span class="citation" data-cites="newman2009 hoffman2010">(<a href="#ref-newman2009" role="doc-biblioref">Newman et al. 2009</a>; <a href="#ref-hoffman2010" role="doc-biblioref">Hoffman, Bach, and Blei 2010</a>)</span>.</p>
<p>LDA is currently the most used algorithm, for its simplicity and speed. Moreover, a lot of different domain specific adaptations have been proposed <span class="citation" data-cites="jelodar2019">(<a href="#ref-jelodar2019" role="doc-biblioref">Jelodar et al. 2019</a>)</span>.</p>
<p>On the other hand, its critics highlight two limits: the first is the absence of experimental data to support the choice of the Dirichlet priors, which additionally prevent the algorithm to be able to fit exactly a word distribution following the Zipf’s law <span class="citation" data-cites="zipf1965">(<a href="#ref-zipf1965" role="doc-biblioref">Zipf 1965</a>)</span> (i.e. a power law or Pareto distribution of the absolute frequencies) <span class="citation" data-cites="gerlach2018">(<a href="#ref-gerlach2018" role="doc-biblioref">Gerlach, Peixoto, and Altmann 2018</a>)</span>; the second is it being a parametric model, but a non-parametric adaptation will be introduced in the next section.</p>
<h2 id="hierarchical-dirichlet-process">Hierarchical Dirichlet Process</h2>
<p>We now describe a first non-parametric model, which can be interpreted as an extension of LDA.</p>
<p>To explain the logic behind the Hierarchical Dirichlet Process (HDP) algorithm <span class="citation" data-cites="teh2005">(<a href="#ref-teh2005" role="doc-biblioref">Teh et al. 2005</a>)</span>, it is useful to introduce the Dirichlet Process (DP).</p>
<p>Following <span class="citation" data-cites="teh2005">Teh et al. (<a href="#ref-teh2005" role="doc-biblioref">2005</a>)</span> a DP can be modelled with the so-called <em>Pólya urn scheme</em> “<em>in which a ball of a distinct color is associated with each [category]. The balls are drawn equiprobably; when a ball is drawn it is placed back in the urn together with another ball of the same color. In addition, with probability proportional to <span class="math inline">\(\alpha_0\)</span> a new [category] is created by drawing from [a given distribution] and a ball of a new color is added to the urn.</em>”. In our context we can use the urn for a single document, the categories for the topics and the balls for the words.</p>
<p>We now have to extend this process to a corpus of documents, and it can be done by concatenating an outer DP which determines the topics (categories), their number and the distribution from which they are drawn, and an inner DP which assigns each word of a document to a topic. This construction allows to share information (the number and the composition of the topics) among otherwise unrelated stochastic generative processes.</p>
<p>As for LDA, also for HDP a Gibbs’ sampling and a variational inference algorithms are available <span class="citation" data-cites="teh2005 wang2011 newman2009">(<a href="#ref-teh2005" role="doc-biblioref">Teh et al. 2005</a>; <a href="#ref-wang2011" role="doc-biblioref">Wang, Paisley, and Blei 2011</a>; <a href="#ref-newman2009" role="doc-biblioref">Newman et al. 2009</a>)</span>.</p>
<h1 id="ch:net">Network-based methods</h1>
<p>The count matrix as defined in section <a href="#sec:count" data-reference-type="ref" data-reference="sec:count">2.3</a> represents also the incidence matrix of a bipartite weighted network<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a><a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> in which the nodes represent the words and the documents, and the links and their weights represent the occurrences of each word in each document<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a>.</p>
<figure>
<img src="fig/hsbm.png" id="fig:hsbm" alt="This figure taken from Gerlach, Peixoto, and Altmann (2018) illustrates very clearly the duality between the matrix representation in terms of frequencies and the graph representation. The second row of the figure represents on the left a NMF (see section 3.2) decomposition, while on the right the SBM (see section 5.2) decomposition of the graph obtained from the adjacency matrix A (see footnote [fn:adj])." /><figcaption aria-hidden="true">This figure taken from <span class="citation" data-cites="gerlach2018">Gerlach, Peixoto, and Altmann (<a href="#ref-gerlach2018" role="doc-biblioref">2018</a>)</span> illustrates very clearly the duality between the matrix representation in terms of frequencies and the graph representation. The second row of the figure represents on the left a NMF (see section <a href="#sec:nmf" data-reference-type="ref" data-reference="sec:nmf">3.2</a>) decomposition, while on the right the SBM (see section <a href="#sec:hsbm" data-reference-type="ref" data-reference="sec:hsbm">5.2</a>) decomposition of the graph obtained from the adjacency matrix <span class="math inline">\(A\)</span> (see footnote <a href="#fn:adj" data-reference-type="ref" data-reference="fn:adj">[fn:adj]</a>).</figcaption>
</figure>
<p>This representation allows to transfer the algorithm developed for community detection to the domain of Topic Models with two constrains: first, each cluster must contain either words or documents and not both (which would be the meaning of a cluster of words and documents?) and only few community detection algorithms deal well with bipartite graphs; second, we are interested in soft-cluster algorithms which assign each node to a cluster with a given probability, but many community detection algorithms perform hard-clustering assigning each node to one and one only cluster.</p>
<p>We have already cited in section <a href="#sec:nmf" data-reference-type="ref" data-reference="sec:nmf">3.2</a> that NMF can be interpreted as the soft-clustering of a bi-partite graph, but its performance is rather poor.</p>
<p>Two are the possibilities: finding a representation of the corpus suitable to be hard-clustered and then recover a soft-cluster classification (like in section <a href="#sec:tm" data-reference-type="ref" data-reference="sec:tm">5.1</a> <span class="citation" data-cites="lancichinetti2015">(<a href="#ref-lancichinetti2015" role="doc-biblioref">Lancichinetti et al. 2015</a>)</span>, <span class="citation" data-cites="kido2016">(<a href="#ref-kido2016" role="doc-biblioref">Kido, Igawa, and Barbon Jr. 2016</a>)</span> or <span class="citation" data-cites="rao2021">(<a href="#ref-rao2021" role="doc-biblioref">Rao and Chakraborty 2021</a>)</span>); using a soft-clustering algorithm which can deal with bipartite graphs (like in section <a href="#sec:hsbm" data-reference-type="ref" data-reference="sec:hsbm">5.2</a> <span class="citation" data-cites="gerlach2018">(<a href="#ref-gerlach2018" role="doc-biblioref">Gerlach, Peixoto, and Altmann 2018</a>)</span>);</p>
<p>Both the two methods we present in this section are non-parametric, as many clustering algorithms.</p>
<h2 id="sec:tm">Topic Mapping</h2>
<p>Developing Topic Mapping, <span class="citation" data-cites="lancichinetti2015">Lancichinetti et al. (<a href="#ref-lancichinetti2015" role="doc-biblioref">2015</a>)</span> followed the way of translating the Topic Model problem in a hard-clustering problem and then processing the result to a soft-clustering model.</p>
<p>In our setting a topic is a group of words which frequently appear together. So, if we can find a way to get a new network composed only by words, whose links are weighted according to the co-occurrences of each couple of words, the communities detected in it can represent a topic.</p>
<p>In fact the first step of the Topic Mapping algorithm is the projection of the bipartite network in a words-only network using as weight of the links the cosine similarity between the distributions among the documents of each couple of words. If two words do not co-appear in any document their cosine similarity is 0 and the link is dropped. Then the obtained graph is compared with a null model in which the words are shuffled among all the documents: the distribution of the weights in the null model is well approximated by a Poisson distribution and only the links with a very implausible weight in the null model (i.e. with a low p-value) are kept.</p>
<p>The graph obtained is clustered using InfoMap <span class="citation" data-cites="rosvall2008 rosvall2009">(<a href="#ref-rosvall2008" role="doc-biblioref">M. Rosvall and Bergstrom 2008</a>; <a href="#ref-rosvall2009" role="doc-biblioref">M. Rosvall, Axelsson, and Bergstrom 2009</a>)</span>, which is a hard-clustering algorithm which uses an information theory approach trying to minimize the information required to describe the graph<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a>. Finally, the clusters with less than a given number of nodes are dropped.</p>
<p>A soft-clustering partition in <span class="math inline">\(K\)</span> classes is a point of the <span class="math inline">\(K-1\)</span> dimensional simplex, but the vertexes of the simplex represent the certainty in the assignment, which is what the result of a hard-clustering algorithm is from the point of view of a soft-clustering algorithm. Then it is possible to encode the topics found with InfoMap as a topic-word matrix with a single one in each column to represent the cluster (topic) to which the word was assigned, and the other elements equal to zero. This matrix is used as starting point for a PLSA-like model<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a> which is further refined using the LDA variational algorithm.</p>
<p>This algorithm can be parallelized except for the InfoMap clustering which is a sequential process<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a>. A parallelized version of the algorithm is not straightforwardly available, even excluding the clustering step.</p>
<h2 id="sec:hsbm">Hierarchical Stochastic Block Model</h2>
<p>The Hierarchical Stochastic Block Model algorithm (hSBM) <span class="citation" data-cites="gerlach2018">(<a href="#ref-gerlach2018" role="doc-biblioref">Gerlach, Peixoto, and Altmann 2018</a>)</span> is an extension of the Stochastic Block Model algorithm (SBM) proposed by <span class="citation" data-cites="ball2011">Ball, Karrer, and Newman (<a href="#ref-ball2011" role="doc-biblioref">2011</a>)</span>.</p>
<p>SBM hypothesizes that in a network with <span class="math inline">\(K\)</span> communities (with <span class="math inline">\(K\)</span> a-priori fixed) each node <span class="math inline">\(i\)</span> has a probability <span class="math inline">\(\theta_{ik}\)</span> to link to another node in the community <span class="math inline">\(k\)</span>, and so the probability to observe a link between the nodes <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> is <span class="math inline">\(\sum_k \theta_{ik} \theta_{jk}\)</span>. If each node has a strong probability to belong to a few communities we can reorder the row of the adjacency matrix (and so the column given its symmetry) to observe some blocks with a high density of zero (the unrealized links among nodes of different communities) and some blocks with a high density of non-zeros values (the communities).</p>
<p>SBM applied to the count matrix is equivalent to PLSA <span class="citation" data-cites="gerlach2018">(<a href="#ref-gerlach2018" role="doc-biblioref">Gerlach, Peixoto, and Altmann 2018</a>)</span> if only the words are clustered, getting the topics as blocks of words.</p>
<p>hSBM extends SBM by explicitly accounting for the bipartite nature of the network and at the same time creating word-only and document-only clusters.</p>
<p>This representation is substantially different from those we have seen until here: the other models all hypothesize that a topic can be defined as a mixture of words, and the goal is to find those mixtures and reconstruct, based on the likelihood, which of those is the more present (or in which measure each of those is present) in a document; hSBM hypothesizes that a topic is more generally what is common among similar documents (in terms of their words’ composition) and that, at the same time, we can find semantically sound groups of words which often occur together.</p>
<p>This changes the way to characterize the topics in terms of groups of words: we can say that while other models describe a document as a mixture of mixtures of words, hSBM describes a document as a mixture of mixtures of mixtures of words. This further level of abstraction allows hSBM to capture richer and more complex topics (on the documents side) and to recognize groups of functionally similar words, for example words that are uninformative because present in every topic, which is the very definition of stopwords given in section  <a href="#sec:sw" data-reference-type="ref" data-reference="sec:sw">2.2.1</a>.</p>
<p>Moreover, hSBM infers a hierarchy of SBMs where the communities can be merged and split among the different levels <span class="citation" data-cites="peixoto2019">(<a href="#ref-peixoto2019" role="doc-biblioref">Peixoto 2019</a>)</span> (getting in this way a non-parametric model) and relaxes the hypothesis of SBM <span class="citation" data-cites="gerlach2018">(<a href="#ref-gerlach2018" role="doc-biblioref">Gerlach, Peixoto, and Altmann 2018</a>)</span>, being able for example to fit a Zipf’s distribution of words’ frequencies, differently from LDA (see section <a href="#sec:lda" data-reference-type="ref" data-reference="sec:lda">4.2</a>).</p>
<p>The model is inferred by using a MCMC algorithm since it translates a network-theory problem in a probabilistic Bayesian one.</p>
<p>It is important to highlight that a hSBM is in fact a collection of models with different levels of resolution, one for each level of the hierarchy: this can be interpreted as the ability to classify, for example, documents from different disciplines and, inside each of them, recognize the different fields of research and, again, for each of them the single research topics.</p>
<h1 id="models-for-corpora-with-metadata">Models for corpora with metadata</h1>
<p>We conclude this section illustrating a recent development of the Topic Modelling problem.</p>
<p>It is very common to have available not only the text of the documents in a corpus but also some complementary data (generally called metadata) such as the authors, the year of publication, hyper-textual links or the citation graph which links the documents together.</p>
<p>The network approach is well suited to be extended by using the concept of multi-layer network, which is a network with a single set of nodes (eventually parted in different groups which represent different entities, like words, documents, authors) but many sets of edges (eventually weighted) each representing a different aspect we want to study <span class="citation" data-cites="hyland2021">(<a href="#ref-hyland2021" role="doc-biblioref">Hyland et al. 2021</a>)</span>.</p>
<p>On the other hand, also a probabilistic approach based on a stochastic generative process like LDA can be enhanced to account for the information coming from metadata <span class="citation" data-cites="rabinovich2014">(<a href="#ref-rabinovich2014" role="doc-biblioref">Rabinovich and Blei 2014</a>)</span>.</p>
<p>Finally, we briefly focus on a particular kind of metadata: time. It is a common situation, for example in history of ideas, to be interested in describing how a textual corpus evolves through time.</p>
<p>Three are the possibilities to tackle this specific problem. The first is to infer a topic model without accounting for time and then using other statistical tools (like a regression) to explore the (cor)relations between time and topics, for example averaging the topic distribution of all the documents in a given year or considering the fraction of documents in a given year with a given topic as the most probable. The second is to develop an algorithm which includes this ordinal information and is able to describe the evolution of topics through time <span class="citation" data-cites="bleiDynamicTopicModels2006">(e.g. <a href="#ref-bleiDynamicTopicModels2006" role="doc-biblioref">Blei and Lafferty 2006</a>)</span>. The last is to divide the corpus in sub-corpora each covering a smaller timespan (overlapping or not) and then finding a way to link the topics coming from different timespans: this kind of problem is studied both in the Topic Model community <span class="citation" data-cites="dicaroBimodalNetworkApproach2017">(e.g. <a href="#ref-dicaroBimodalNetworkApproach2017" role="doc-biblioref">Di Caro et al. 2017</a>)</span> and in the community detection (a subfield of network theory) one <span class="citation" data-cites="rosvall2010">(e.g. <a href="#ref-rosvall2010" role="doc-biblioref">Martin Rosvall and Bergstrom 2010</a>)</span>.</p>
<h1 id="compare-topic-models">Compare Topic Models</h1>
<p>In the first part we described some algorithms to infer a Topic Model, but we have not provided a clear criterion of choice. In fact a consensus in literature on which is the best suited algorithm for different situations, is still to be reached.</p>
<p>Moreover, there is no consensus on how to compare different topic models or to asses their goodness. For probabilistic models the likelihood of the model can be a guide, but it suffers from a sensitivity on the number of parameters to be inferred (see section <a href="#sec:plsa" data-reference-type="ref" data-reference="sec:plsa">4.1</a>), similarly linear algebra methods can be compared by their reconstruction error. But neither of these is a model-agnostic framework of comparison.</p>
<p>We can divide the techniques proposed in literature in three groups: comparison on synthetic corpora, comparison on annotated corpora, comparison of the <em>topic coherence</em> of inferred models.</p>
<p>With <em>topic coherence</em> we indicate a set of measures which aims to describe the internal coherence of the topics given a reference corpus, the one used to infer the model or a different one <span class="citation" data-cites="roder2015">(<a href="#ref-roder2015" role="doc-biblioref">Röder, Both, and Hinneburg 2015</a>)</span>. Generally speaking this techniques take the most probable words in a topic and assess if in the reference corpus they are likely to appear in the same documents and possibly close (i.e. in the same n-gram, for small values of n). While they give insights on the likeliness of the topic inferred, they conveys little information on how well the documents are classified and heavily depend on the corpus chosen to calculate the metric. However, this kind of assessment does not require a human-classification of the original corpus, making it the only possible choice in an unsupervised setting.</p>
<p>A synthetic corpus is a corpus generated according to a given stochastic process, for example the one hypothesized by LDA (see section <a href="#sec:lda" data-reference-type="ref" data-reference="sec:lda">4.2</a>), using symbolic tokens, like numbers, as words and assigning each document to a true mixture of topics which the model has to infer. It is very useful to asses if a model which aims to describe a stochastic process is in fact able to do that, but it is debatable which synthetic corpora are similar enough to real corpora to provide a good benchmark. An example of benchmark realized with this technique is presented in <span class="citation" data-cites="shiNewEvaluationFramework2019">Shi et al. (<a href="#ref-shiNewEvaluationFramework2019" role="doc-biblioref">2019</a>)</span>.</p>
<p>The last strategy requires a classified corpus to be available. Some corpora are relatively easy to be obtained with a classification, for example documents written in different languages, documents retrieved by organized collections like Wikipedia, Scopus or Web of Science, messages sent in similar but theme-specific newsgroups or bulletin-board. Some research groups have produced human-classified corpora (even at sentence levels) <span class="citation" data-cites="merzManifestoCorpusNew2016">(e.g. <a href="#ref-merzManifestoCorpusNew2016" role="doc-biblioref">Merz, Regel, and Lewandowski 2016</a>)</span> while others have tried to extend human classification to large corpora using machine learning techniques <span class="citation" data-cites="angrist2017">(e.g. <a href="#ref-angrist2017" role="doc-biblioref">Angrist et al. 2017</a>)</span>.</p>
<p>We have chosen to follow the strategy of the comparison with annotated corpora and in the next chapters we describe how the comparison was made and the results obtained.</p>
<h2 id="measures">Measures</h2>
<p>In the setting we have chosen, we have to assess if a model is able to classify a document in the correct group represented by the label assigned in the original corpus, comparing it with the topic assigned by the Topic Model.</p>
<p>A problem with this task is that we do not know which topic better represents each label. For this reason we looked for a measure of similarity which is independent from the order in which topics and labels are listed.</p>
<p>Furthermore, we did not enforce that the topics inferred will be the same number of the labels, and for non-parametric models doing this is in fact impossible and meaningless, and so we need a measure which is able to compare a different number of topics and labels.</p>
<p>Moreover, we compared really different models, and so we needed a measure that could be easily interpreted with little to no context. Generally, normalized measures which have a maximum value (perfect match) and a minimum value (no match at all) are used in these cases.</p>
<p>Finally, we do not have a classification of a document in a topic, even if it can be obtained by choosing the topic with the maximum score (or probability), but rather, a distribution of probabilities to belongs to a topic. This situation is common in community detection benchmarks for soft-clustering algorithms.</p>
<p>A measure with all these features is the Normalized Mutual Information <span class="citation" data-cites="vinh2010">(<a href="#ref-vinh2010" role="doc-biblioref">Vinh, Epps, and Bailey 2010</a>)</span> and particularly its extension to soft-clustering <span class="citation" data-cites="lei2014">(<a href="#ref-lei2014" role="doc-biblioref">Lei et al. 2014</a>)</span>, which allows to quantify how similar two distributions are.</p>
<p>We will also use this measure to compare the similarity of the predictions of two models, by using one of the two predictions in place of the labels.</p>
<h3 id="normalized-mutual-information">Normalized Mutual Information</h3>
<p>Information theory relies on the concept of entropy <span class="math inline">\(H(\{p_i\}) = \sum_i p_i \log p_i\)</span>, where <span class="math inline">\(\{p_i\}\)</span> is a probability distribution. Entropy has values in <span class="math inline">\([0,1]\)</span> and particularly <span class="math inline">\(H(\{p_i\}) = 0\)</span> for a uniform distribution and <span class="math inline">\(H(\{p_i\}) = 1\)</span> for a degenerate distribution (i.e. with only one non-null value). In other words, entropy is 1 when we have a certainty of the outcome (i.e. the information on the phenomenon is complete) and 0 when the is no information available on the outcome (i.e. every outcome is equiprobable).</p>
<p>We can try to extend the idea of entropy to a new measure which, given two probability distributions, expresses how much one helps us to describe the other, i.e. how much information on the outcomes is shared between the two distributions<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a>.</p>
<p>Calling <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{Y}\)</span> two random variables we define the mutual information as <span class="math display">\[\begin{aligned}
    MI(\mathbf{X},\mathbf{Y}) = &amp;\sum_{\mathbf{X},\mathbf{Y}} \mathbb{P}(\mathbf{X}\cap\mathbf{Y})\log\frac{\mathbb{P}(\mathbf{X}\cap\mathbf{Y})}{\mathbb{P}(\mathbf{X})\mathbb{P}(\mathbf{Y})} \\
    = &amp;-\sum_{\mathbf{X},\mathbf{Y}} \mathbb{P}(\mathbf{X}\cap\mathbf{Y})\log\left(\mathbb{P}(\mathbf{X})\mathbb{P}(\mathbf{Y})\right) + \sum_{\mathbf{X},\mathbf{Y}} \mathbb{P}(\mathbf{X}\cap\mathbf{Y})\log{\mathbb{P}(\mathbf{X}\cap\mathbf{Y})} \\
    = &amp;-\sum_{\mathbf{X},\mathbf{Y}} \mathbb{P}(\mathbf{X}\cap\mathbf{Y})\log\left(\mathbb{P}(\mathbf{X})\mathbb{P}(\mathbf{Y})\right) - H(\mathbf{X} \cap \mathbf{Y})\end{aligned}\]</span> which is (a weighted average of) the logarithm of the ratio between the observed joint distribution and the hypothetical joint distribution if the two random variables were independent, using the marginal distributions as true distributions. Note that this definition is symmetric with regard to the exchange of the two random variables.</p>
<p>Particularly, if two random variables are independent, which means that we cannot get any information on one observing the other, the ratio is equal to one and so <span class="math inline">\(MI=0\)</span>. Moreover, since <span class="math inline">\(\mathbb{P}(\mathbf{X}\cap\mathbf{Y}) \geq \mathbb{P}(\mathbf{X})\mathbb{P}(\mathbf{Y})\)</span><a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a>, <span class="math inline">\(MI\geq0\)</span> and it can be interpreted as the information we gain knowing the joint distribution rather than the marginal ones.</p>
<p>This definition is not apparently superior limited, which makes it difficult to compare this measure among different models. But it is possible to show that <span class="math inline">\(MI(\mathbf{X},\mathbf{Y}) \leq \min\{H(\mathbf{X}),H(\mathbf{Y})\}\)</span><a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a> and so it is possible to normalize the mutual information to have values in <span class="math inline">\([0,1]\)</span>. Among the possible choices of normalization <span class="citation" data-cites="vinh2010">(see <a href="#ref-vinh2010" role="doc-biblioref">Vinh, Epps, and Bailey 2010</a>)</span> we have chosen to define the normalized mutual information (NMI) as <span class="math display">\[NMI(\mathbf{X},\mathbf{Y}) = \frac{MI(\mathbf{X},\mathbf{Y})}{H(\mathbf{X}) + H(\mathbf{Y})}\]</span></p>
<p>Note that to compute <span class="math inline">\(MI\)</span> only the joint distribution of <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{Y}\)</span> is required, from which we can compute the marginal ones.</p>
<p>In our case of interest, the two distributions are the number of documents which show a given topic as the most probable and the number of documents belonging to a given category, adequately normalized to sum to 1.</p>
<p>We are in fact interested in the so-called contingency table <span class="math inline">\(T\)</span> which is a matrix with values in <span class="math inline">\(\mathbb{N}\)</span> with dimensions the number of topics times the number of labels where each entry is the number of documents with the given labels and the given topic as the most probable.</p>
<p>From the contingency table (which is the non-normalized joint distribution) we can recover the marginal distributions by summing on the rows or the columns and then normalizing.</p>
<p>This formulation is well suited for exclusive classification, in which each document is assigned to one topic (the most probable one). We now will describe how to extend this measure to probabilistic classifications.</p>
<h4 id="extension-to-probabilistic-classification">Extension to probabilistic classification</h4>
<p>The contingency table <span class="math inline">\(T\)</span> with dimensions <span class="math inline">\(m \times n\)</span> can be written as <span class="math inline">\(T = A_1^T B_1\)</span> where <span class="math inline">\(A_1\)</span> and <span class="math inline">\(B_1\)</span> are two matrices with dimensions number of records (the documents in our case) and, respectively, <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> (the number of topics and labels) whose values are 1 if the record is assigned to the class correspondent to the column and 0 otherwise. This is the so called one-hot representation of a classification, in which for each record is defined a vector with a single 1 for the right class and 0s for all the others.</p>
<p>Following <span class="citation" data-cites="lei2014">Lei et al. (<a href="#ref-lei2014" role="doc-biblioref">2014</a>)</span> if we substitute the matrix <span class="math inline">\(A_1\)</span> and <span class="math inline">\(B_1\)</span> with two matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> in which the rows are the probability distributions associated to the classification (i.e. the certainty of the classification), we obtain <span class="math inline">\(T&#39; = A^T B\)</span>. This matrix do not have an immediate interpretation but if we preserve the one hot distribution for the matrix <span class="math inline">\(B\)</span> (which in our case represents the assignment of a document to a label, which is a priori determined) <span class="math inline">\(T&#39;\)</span> is obtained by weighting each occurrence in the contingency table with the degree of certainty we have about it before summing, while <span class="math inline">\(T\)</span> is obtained by simply counting (i.e. giving weight 1 to every record).</p>
<h2 id="example-corpora">Example corpora</h2>
<p>To realize our benchmark we chose two corpora: 20Newsgroups (a dataset of messages sent into twenty newsgroups)<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a> and a collection of abstracts from Web of Science (WOS-46985 or wos)<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a>. 20Newsgroups is labelled by the newsgroup (out of 20) in which each message was sent, while WOS-46985 has two levels of labels: 7 categories and 134 sub-categories.</p>
<p>Many other possibilities are available for further studies<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a>.</p>
<div class="refcontext">

</div>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-akaike1974" class="csl-entry" role="doc-biblioentry">
Akaike, H. 1974. <span>“A New Look at the Statistical Model Identification.”</span> <em>IEEE Trans. Automat. Contr.</em> 19 (6): 716–23. <a href="https://doi.org/10.1109/TAC.1974.1100705">https://doi.org/10.1109/TAC.1974.1100705</a>.
</div>
<div id="ref-angelov2020" class="csl-entry" role="doc-biblioentry">
Angelov, Dimo. 2020. <span>“<span>Top2Vec</span>: Distributed <span>Representations</span> of <span>Topics</span>.”</span> August 19, 2020. <a href="http://arxiv.org/abs/2008.09470">http://arxiv.org/abs/2008.09470</a>.
</div>
<div id="ref-angrist2017" class="csl-entry" role="doc-biblioentry">
Angrist, Joshua, Pierre Azoulay, Glenn Ellison, Ryan Hill, and Susan Feng Lu. 2017. <span>“Economic <span>Research Evolves</span>: Fields and <span>Styles</span>.”</span> <em>American Economic Review</em> 107 (5, 5): 293–97. <a href="https://doi.org/gdgx8x">https://doi.org/gdgx8x</a>.
</div>
<div id="ref-arora2012" class="csl-entry" role="doc-biblioentry">
Arora, Sanjeev, Rong Ge, Yoni Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu, and Michael Zhu. 2012. <span>“A <span>Practical Algorithm</span> for <span>Topic Modeling</span> with <span>Provable Guarantees</span>.”</span> December 19, 2012. <a href="http://arxiv.org/abs/1212.4777">http://arxiv.org/abs/1212.4777</a>.
</div>
<div id="ref-ball2011" class="csl-entry" role="doc-biblioentry">
Ball, Brian, Brian Karrer, and M. E. J. Newman. 2011. <span>“Efficient and Principled Method for Detecting Communities in Networks.”</span> <em>Phys. Rev. E</em> 84 (3): 036103. <a href="https://doi.org/10.1103/PhysRevE.84.036103">https://doi.org/10.1103/PhysRevE.84.036103</a>.
</div>
<div id="ref-berry2007" class="csl-entry" role="doc-biblioentry">
Berry, Michael W., Murray Browne, Amy N. Langville, V. Paul Pauca, and Robert J. Plemmons. 2007. <span>“Algorithms and Applications for Approximate Nonnegative Matrix Factorization.”</span> <em>Computational Statistics &amp; Data Analysis</em> 52 (1): 155–73. <a href="https://doi.org/10.1016/j.csda.2006.11.006">https://doi.org/10.1016/j.csda.2006.11.006</a>.
</div>
<div id="ref-bleiDynamicTopicModels2006" class="csl-entry" role="doc-biblioentry">
Blei, David M., and John D. Lafferty. 2006. <span>“Dynamic Topic Models.”</span> In <em>Proceedings of the 23rd International Conference on <span>Machine</span> Learning - <span>ICML</span> ’06</em>, 113–20. <span>Pittsburgh, Pennsylvania</span>: <span>ACM Press</span>. <a href="https://doi.org/bjkmq9">https://doi.org/bjkmq9</a>.
</div>
<div id="ref-blei2003" class="csl-entry" role="doc-biblioentry">
Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. <span>“Latent Dirichlet Allocation.”</span> <em>J. Mach. Learn. Res.</em> 3 (March): 993–1022.
</div>
<div id="ref-campello2013" class="csl-entry" role="doc-biblioentry">
Campello, Ricardo J. G. B., Davoud Moulavi, and Joerg Sander. 2013. <span>“Density-<span>Based Clustering Based</span> on <span>Hierarchical Density Estimates</span>.”</span> In <em>Advances in <span>Knowledge Discovery</span> and <span>Data Mining</span></em>, edited by Jian Pei, Vincent S. Tseng, Longbing Cao, Hiroshi Motoda, and Guandong Xu, 7819:160–72. Lecture <span>Notes</span> in <span>Computer Science</span>. <span>Berlin, Heidelberg</span>: <span>Springer Berlin Heidelberg</span>. <a href="https://doi.org/10.1007/978-3-642-37456-2_14">https://doi.org/10.1007/978-3-642-37456-2_14</a>.
</div>
<div id="ref-deerwester1990" class="csl-entry" role="doc-biblioentry">
Deerwester, Scott, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. <span>“Indexing by Latent Semantic Analysis.”</span> <em>Journal of the American Society for Information Science</em> 41 (6): 391–407. <a href="https://doi.org/10.1002/(SICI)1097-4571(199009)41:6&lt;391::AID-ASI1&gt;3.0.CO;2-9">https://doi.org/10.1002/(SICI)1097-4571(199009)41:6&lt;391::AID-ASI1&gt;3.0.CO;2-9</a>.
</div>
<div id="ref-dicaroBimodalNetworkApproach2017" class="csl-entry" role="doc-biblioentry">
Di Caro, Luigi, Marco Guerzoni, Massimiliano Nuccio, and Giovanni Siragusa. 2017. <span>“A <span>Bimodal Network Approach</span> to <span>Model Topic Dynamics</span>.”</span> September 27, 2017. <a href="http://arxiv.org/abs/1709.09373">http://arxiv.org/abs/1709.09373</a>.
</div>
<div id="ref-ding2005" class="csl-entry" role="doc-biblioentry">
Ding, Chris, Xiaofeng He, and Horst D. Simon. 2005. <span>“On the <span>Equivalence</span> of <span>Nonnegative Matrix Factorization</span> and <span>Spectral Clustering</span>.”</span> In <em>Proceedings of the 2005 <span>SIAM International Conference</span> on <span>Data Mining</span> (<span>SDM</span>)</em>, 606–10. Proceedings. <span>Society for Industrial and Applied Mathematics</span>. <a href="https://doi.org/10.1137/1.9781611972757.70">https://doi.org/10.1137/1.9781611972757.70</a>.
</div>
<div id="ref-eckart1936" class="csl-entry" role="doc-biblioentry">
Eckart, Carl, and Gale Young. 1936. <span>“The Approximation of One Matrix by Another of Lower Rank.”</span> <em>Psychometrika</em> 1 (3): 211–18. <a href="https://doi.org/10.1007/BF02288367">https://doi.org/10.1007/BF02288367</a>.
</div>
<div id="ref-gaussier2005" class="csl-entry" role="doc-biblioentry">
Gaussier, Eric, and Cyril Goutte. 2005. <span>“Relation Between <span>PLSA</span> and <span>NMF</span> and Implications.”</span> In <em>Proceedings of the 28th Annual International <span>ACM SIGIR</span> Conference on <span>Research</span> and Development in Information Retrieval - <span>SIGIR</span> ’05</em>, 601. <span>Salvador, Brazil</span>: <span>ACM Press</span>. <a href="https://doi.org/10.1145/1076034.1076148">https://doi.org/10.1145/1076034.1076148</a>.
</div>
<div id="ref-gerlachStandardizedProjectGutenberg2018" class="csl-entry" role="doc-biblioentry">
Gerlach, Martin, and Francesc Font-Clos. 2018. <span>“A Standardized <span>Project Gutenberg</span> Corpus for Statistical Analysis of Natural Language and Quantitative Linguistics.”</span> December 19, 2018. <a href="http://arxiv.org/abs/1812.08092">http://arxiv.org/abs/1812.08092</a>.
</div>
<div id="ref-gerlach2018" class="csl-entry" role="doc-biblioentry">
Gerlach, Martin, Tiago P. Peixoto, and Eduardo G. Altmann. 2018. <span>“A Network Approach to Topic Models.”</span> <em>Sci. Adv.</em> 4 (7, 7): eaaq1360. <a href="https://doi.org/gdxnxq">https://doi.org/gdxnxq</a>.
</div>
<div id="ref-gerlach2019" class="csl-entry" role="doc-biblioentry">
Gerlach, Martin, Hanyu Shi, and Luís A. Nunes Amaral. 2019. <span>“A Universal Information Theoretic Approach to the Identification of Stopwords.”</span> <em>Nat Mach Intell</em> 1 (12): 606–12. <a href="https://doi.org/10.1038/s42256-019-0112-6">https://doi.org/10.1038/s42256-019-0112-6</a>.
</div>
<div id="ref-griffiths2004" class="csl-entry" role="doc-biblioentry">
Griffiths, T. L., and M. Steyvers. 2004. <span>“Finding Scientific Topics.”</span> <em>Proceedings of the National Academy of Sciences</em> 101 (April): 5228–35. <a href="https://doi.org/10.1073/pnas.0307752101">https://doi.org/10.1073/pnas.0307752101</a>.
</div>
<div id="ref-grootendorst2021" class="csl-entry" role="doc-biblioentry">
Grootendorst, Maarten, and Nils Reimers. 2021. <em><span>MaartenGr</span>/<span>BERTopic</span>: V0.9.1</em> (version v0.9.1). <span>Zenodo</span>. <a href="https://doi.org/10.5281/ZENODO.4381785">https://doi.org/10.5281/ZENODO.4381785</a>.
</div>
<div id="ref-hoffman2010" class="csl-entry" role="doc-biblioentry">
Hoffman, Matthew, Francis Bach, and David Blei. 2010. <span>“Online <span>Learning</span> for <span>Latent Dirichlet Allocation</span>.”</span> In <em>Advances in <span>Neural Information Processing Systems</span></em>. Vol. 23. <span>Curran Associates, Inc.</span> <a href="https://papers.nips.cc/paper/2010/hash/71f6278d140af599e06ad9bf1ba03cb0-Abstract.html">https://papers.nips.cc/paper/2010/hash/71f6278d140af599e06ad9bf1ba03cb0-Abstract.html</a>.
</div>
<div id="ref-hofmann1999" class="csl-entry" role="doc-biblioentry">
Hofmann, Thomas. 1999. <span>“Probabilistic <span>Latent Semantic Analysis</span>.”</span> 1999. <a href="http://arxiv.org/abs/1301.6705">http://arxiv.org/abs/1301.6705</a>.
</div>
<div id="ref-hsieh2011" class="csl-entry" role="doc-biblioentry">
Hsieh, Cho-Jui, and Inderjit S. Dhillon. 2011. <span>“Fast Coordinate Descent Methods with Variable Selection for Non-Negative Matrix Factorization.”</span> In <em>Proceedings of the 17th <span>ACM SIGKDD</span> International Conference on <span>Knowledge</span> Discovery and Data Mining - <span>KDD</span> ’11</em>, 1064. <span>San Diego, California, USA</span>: <span>ACM Press</span>. <a href="https://doi.org/10.1145/2020408.2020577">https://doi.org/10.1145/2020408.2020577</a>.
</div>
<div id="ref-hyland2021" class="csl-entry" role="doc-biblioentry">
Hyland, Charles C., Yuanming Tao, Lamiae Azizi, Martin Gerlach, Tiago P. Peixoto, and Eduardo G. Altmann. 2021. <span>“Multilayer Networks for Text Analysis with Multiple Data Types.”</span> <em>EPJ Data Sci.</em> 10 (1): 33. <a href="https://doi.org/10.1140/epjds/s13688-021-00288-5">https://doi.org/10.1140/epjds/s13688-021-00288-5</a>.
</div>
<div id="ref-jelodar2019" class="csl-entry" role="doc-biblioentry">
Jelodar, Hamed, Yongli Wang, Chi Yuan, Xia Feng, Xiahui Jiang, Yanchao Li, and Liang Zhao. 2019. <span>“Latent <span>Dirichlet</span> Allocation (<span>LDA</span>) and Topic Modeling: Models, Applications, a Survey.”</span> <em>Multimed Tools Appl</em> 78 (11): 15169–211. <a href="https://doi.org/10.1007/s11042-018-6894-4">https://doi.org/10.1007/s11042-018-6894-4</a>.
</div>
<div id="ref-kang2018" class="csl-entry" role="doc-biblioentry">
Kang, Mangi, Jaelim Ahn, and Kichun Lee. 2018. <span>“Opinion Mining Using Ensemble Text Hidden <span>Markov</span> Models for Text Classification.”</span> <em>Expert Systems with Applications</em> 94 (March): 218–27. <a href="https://doi.org/10.1016/j.eswa.2017.07.019">https://doi.org/10.1016/j.eswa.2017.07.019</a>.
</div>
<div id="ref-kido2016" class="csl-entry" role="doc-biblioentry">
Kido, Guilherme Sakaji, Rodrigo Augusto Igawa, and Sylvio Barbon Jr. 2016. <span>“Topic <span>Modeling</span> Based on <span>Louvain</span> Method in <span>Online Social Networks</span>.”</span> In <em>Anais Do <span>Simpósio Brasileiro</span> de <span>Sistemas</span> de <span>Informação</span> (<span>SBSI</span>)</em>, 353–60. <span>Sociedade Brasileira de Computação</span>. <a href="https://doi.org/10.5753/sbsi.2016.5982">https://doi.org/10.5753/sbsi.2016.5982</a>.
</div>
<div id="ref-kim2008" class="csl-entry" role="doc-biblioentry">
Kim, Jingu, and Haesun Park. 2008. <span>“Sparse <span>Nonnegative Matrix Factorization</span> for <span>Clustering</span>.”</span> Technical Report. <span>Georgia Institute of Technology</span>. <a href="https://smartech.gatech.edu/handle/1853/20058">https://smartech.gatech.edu/handle/1853/20058</a>.
</div>
<div id="ref-kowsari2017" class="csl-entry" role="doc-biblioentry">
Kowsari, Kamran, Donald E. Brown, Mojtaba Heidarysafa, Kiana Jafari Meimandi, Matthew S. Gerber, and Laura E. Barnes. 2017. <span>“<span>HDLTex</span>: Hierarchical <span>Deep Learning</span> for <span>Text Classification</span>.”</span> <em>2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)</em>, December, 364–71. <a href="https://doi.org/10.1109/ICMLA.2017.0-134">https://doi.org/10.1109/ICMLA.2017.0-134</a>.
</div>
<div id="ref-lancichinetti2015" class="csl-entry" role="doc-biblioentry">
Lancichinetti, Andrea, M. Irmak Sirer, Jane X. Wang, Daniel Acuna, Konrad Körding, and Luís A. Nunes Amaral. 2015. <span>“High-<span>Reproducibility</span> and <span>High</span>-<span>Accuracy Method</span> for <span>Automated Topic Classification</span>.”</span> <em>Phys. Rev. X</em> 5 (1, 1): 011007. <a href="https://doi.org/gfrbjc">https://doi.org/gfrbjc</a>.
</div>
<div id="ref-langville2014" class="csl-entry" role="doc-biblioentry">
Langville, Amy N., Carl D. Meyer, Russell Albright, James Cox, and David Duling. 2014. <span>“Algorithms, <span>Initializations</span>, and <span>Convergence</span> for the <span>Nonnegative Matrix Factorization</span>.”</span> July 27, 2014. <a href="http://arxiv.org/abs/1407.7299">http://arxiv.org/abs/1407.7299</a>.
</div>
<div id="ref-lee2000" class="csl-entry" role="doc-biblioentry">
Lee, Daniel D., and H. Sebastian Seung. 2000. <span>“Algorithms for Non-Negative Matrix Factorization.”</span> In <em>Proceedings of the 13th <span>International Conference</span> on <span>Neural Information Processing Systems</span></em>, 535–41. <span>NIPS</span>’00. <span>Cambridge, MA, USA</span>: <span>MIT Press</span>.
</div>
<div id="ref-lei2014" class="csl-entry" role="doc-biblioentry">
Lei, Yang, James C. Bezdek, Jeffrey Chan, Nguyen Xuan Vinh, Simone Romano, and James Bailey. 2014. <span>“Generalized Information Theoretic Cluster Validity Indices for Soft Clusterings.”</span> In <em>2014 <span>IEEE Symposium</span> on <span>Computational Intelligence</span> and <span>Data Mining</span> (<span>CIDM</span>)</em>, 24–31. <span>Orlando, FL, USA</span>: <span>IEEE</span>. <a href="https://doi.org/10.1109/CIDM.2014.7008144">https://doi.org/10.1109/CIDM.2014.7008144</a>.
</div>
<div id="ref-mcinnes2020" class="csl-entry" role="doc-biblioentry">
McInnes, Leland, John Healy, and James Melville. 2020. <span>“<span>UMAP</span>: Uniform <span>Manifold Approximation</span> and <span>Projection</span> for <span>Dimension Reduction</span>.”</span> September 17, 2020. <a href="http://arxiv.org/abs/1802.03426">http://arxiv.org/abs/1802.03426</a>.
</div>
<div id="ref-merzManifestoCorpusNew2016" class="csl-entry" role="doc-biblioentry">
Merz, Nicolas, Sven Regel, and Jirka Lewandowski. 2016. <span>“The <span>Manifesto Corpus</span>: A New Resource for Research on Political Parties and Quantitative Text Analysis.”</span> <em>Research &amp; Politics</em> 3 (2, 2): 205316801664334. <a href="https://doi.org/gftn2f">https://doi.org/gftn2f</a>.
</div>
<div id="ref-mikolov2013" class="csl-entry" role="doc-biblioentry">
Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. <span>“Efficient <span>Estimation</span> of <span>Word Representations</span> in <span>Vector Space</span>.”</span> September 6, 2013. <a href="http://arxiv.org/abs/1301.3781">http://arxiv.org/abs/1301.3781</a>.
</div>
<div id="ref-miller1995" class="csl-entry" role="doc-biblioentry">
Miller, George A. 1995. <span>“<span>WordNet</span>: A Lexical Database for <span>English</span>.”</span> <em>Commun. ACM</em> 38 (11): 39–41. <a href="https://doi.org/10.1145/219717.219748">https://doi.org/10.1145/219717.219748</a>.
</div>
<div id="ref-newman2009" class="csl-entry" role="doc-biblioentry">
Newman, David, Arthur Asuncion, Padhraic Smyth, and Max Welling. 2009. <span>“Distributed <span>Algorithms</span> for <span>Topic Models</span>.”</span> <em>Journal of Machine Learning Research</em> 10 (62): 1801–28. <a href="http://jmlr.org/papers/v10/newman09a.html">http://jmlr.org/papers/v10/newman09a.html</a>.
</div>
<div id="ref-peixoto2019" class="csl-entry" role="doc-biblioentry">
Peixoto, Tiago P. 2019. <span>“Bayesian Stochastic Blockmodeling.”</span> November 23, 2019. <a href="https://doi.org/10.1002/9781119483298.ch11">https://doi.org/10.1002/9781119483298.ch11</a>.
</div>
<div id="ref-porter1980" class="csl-entry" role="doc-biblioentry">
Porter, M. F. 1980. <span>“An Algorithm for Suffix Stripping.”</span> <em>Program</em> 14 (3): 130–37. <a href="https://doi.org/10.1108/eb046814">https://doi.org/10.1108/eb046814</a>.
</div>
<div id="ref-rabinovich2014" class="csl-entry" role="doc-biblioentry">
Rabinovich, Maxim, and David Blei. 2014. <span>“The <span>Inverse Regression Topic Model</span>.”</span> In <em>International <span>Conference</span> on <span>Machine Learning</span></em>, 199–207. <span>PMLR</span>. <a href="http://proceedings.mlr.press/v32/rabinovich14.html">http://proceedings.mlr.press/v32/rabinovich14.html</a>.
</div>
<div id="ref-rao2021" class="csl-entry" role="doc-biblioentry">
Rao, Rajesh N., and Manojit Chakraborty. 2021. <span>“<span>Vec2GC</span> – <span>A Graph Based Clustering Method</span> for <span>Text Representations</span>.”</span> April 15, 2021. <a href="http://arxiv.org/abs/2104.09439">http://arxiv.org/abs/2104.09439</a>.
</div>
<div id="ref-roder2015" class="csl-entry" role="doc-biblioentry">
Röder, Michael, Andreas Both, and Alexander Hinneburg. 2015. <span>“Exploring the <span>Space</span> of <span>Topic Coherence Measures</span>.”</span> In <em>Proceedings of the <span>Eighth ACM International Conference</span> on <span>Web Search</span> and <span>Data Mining</span></em>, 399–408. <span>Shanghai China</span>: <span>ACM</span>. <a href="https://doi.org/10.1145/2684822.2685324">https://doi.org/10.1145/2684822.2685324</a>.
</div>
<div id="ref-rosvall2010" class="csl-entry" role="doc-biblioentry">
Rosvall, Martin, and Carl T. Bergstrom. 2010. <span>“Mapping <span>Change</span> in <span>Large Networks</span>.”</span> Edited by Fabio Rapallo. <em>PLoS ONE</em> 5 (1): e8694. <a href="https://doi.org/10.1371/journal.pone.0008694">https://doi.org/10.1371/journal.pone.0008694</a>.
</div>
<div id="ref-rosvall2009" class="csl-entry" role="doc-biblioentry">
Rosvall, M., D. Axelsson, and C. T. Bergstrom. 2009. <span>“The Map Equation.”</span> <em>Eur. Phys. J. Spec. Top.</em> 178 (1): 13–23. <a href="https://doi.org/10.1140/epjst/e2010-01179-1">https://doi.org/10.1140/epjst/e2010-01179-1</a>.
</div>
<div id="ref-rosvall2008" class="csl-entry" role="doc-biblioentry">
Rosvall, M., and C. T. Bergstrom. 2008. <span>“Maps of Random Walks on Complex Networks Reveal Community Structure.”</span> <em>Proceedings of the National Academy of Sciences</em> 105 (4): 1118–23. <a href="https://doi.org/10.1073/pnas.0706851105">https://doi.org/10.1073/pnas.0706851105</a>.
</div>
<div id="ref-salton1988" class="csl-entry" role="doc-biblioentry">
Salton, Gerard, and Christopher Buckley. 1988. <span>“Term-Weighting Approaches in Automatic Text Retrieval.”</span> <em>Information Processing &amp; Management</em> 24 (5): 513–23. <a href="https://doi.org/10.1016/0306-4573(88)90021-0">https://doi.org/10.1016/0306-4573(88)90021-0</a>.
</div>
<div id="ref-schofield2017" class="csl-entry" role="doc-biblioentry">
Schofield, Alexandra, Måns Magnusson, and David Mimno. 2017. <span>“Pulling <span>Out</span> the <span>Stops</span>: Rethinking <span>Stopword Removal</span> for <span>Topic Models</span>.”</span> In <em>Proceedings of the 15th <span>Conference</span> of the <span>European Chapter</span> of the <span>Association</span> for <span>Computational Linguistics</span>: Volume 2, <span>Short Papers</span></em>, 432–36. <span>Valencia, Spain</span>: <span>Association for Computational Linguistics</span>. <a href="https://aclanthology.org/E17-2069">https://aclanthology.org/E17-2069</a>.
</div>
<div id="ref-schofield2016" class="csl-entry" role="doc-biblioentry">
Schofield, Alexandra, and David Mimno. 2016. <span>“Comparing <span>Apples</span> to <span>Apple</span>: The <span>Effects</span> of <span>Stemmers</span> on <span>Topic Models</span>.”</span> <em>TACL</em> 4 (December): 287–300. <a href="https://doi.org/10.1162/tacl_a_00099">https://doi.org/10.1162/tacl_a_00099</a>.
</div>
<div id="ref-shiNewEvaluationFramework2019" class="csl-entry" role="doc-biblioentry">
Shi, Hanyu, Martin Gerlach, Isabel Diersen, Doug Downey, and Luis A. N. Amaral. 2019. <span>“A New Evaluation Framework for Topic Modeling Algorithms Based on Synthetic Corpora.”</span> January 28, 2019. <a href="http://arxiv.org/abs/1901.09848">http://arxiv.org/abs/1901.09848</a>.
</div>
<div id="ref-teh2005" class="csl-entry" role="doc-biblioentry">
Teh, Yee, Michael Jordan, Matthew Beal, and David Blei. 2005. <span>“Sharing <span>Clusters</span> Among <span>Related Groups</span>: Hierarchical <span>Dirichlet Processes</span>.”</span> In <em>Advances in <span>Neural Information Processing Systems</span></em>. Vol. 17. <span>MIT Press</span>. <a href="https://papers.nips.cc/paper/2004/hash/fb4ab556bc42d6f0ee0f9e24ec4d1af0-Abstract.html">https://papers.nips.cc/paper/2004/hash/fb4ab556bc42d6f0ee0f9e24ec4d1af0-Abstract.html</a>.
</div>
<div id="ref-vinh2010" class="csl-entry" role="doc-biblioentry">
Vinh, Nguyen Xuan, Julien Epps, and James Bailey. 2010. <span>“Information <span>Theoretic Measures</span> for <span>Clusterings Comparison</span>: Variants, <span>Properties</span>, <span>Normalization</span> and <span>Correction</span> for <span>Chance</span>.”</span> <em>J. Mach. Learn. Res.</em> 11 (December): 2837–54.
</div>
<div id="ref-wallach2009" class="csl-entry" role="doc-biblioentry">
Wallach, Hanna, David Mimno, and Andrew McCallum. 2009. <span>“Rethinking <span>LDA</span>: Why <span>Priors Matter</span>.”</span> In <em>Advances in <span>Neural Information Processing Systems</span></em>. Vol. 22. <span>Curran Associates, Inc.</span> <a href="https://papers.nips.cc/paper/2009/hash/0d0871f0806eae32d30983b62252da50-Abstract.html">https://papers.nips.cc/paper/2009/hash/0d0871f0806eae32d30983b62252da50-Abstract.html</a>.
</div>
<div id="ref-wang2011" class="csl-entry" role="doc-biblioentry">
Wang, Chong, John Paisley, and David Blei. 2011. <span>“Online <span>Variational Inference</span> for the <span>Hierarchical Dirichlet Process</span>.”</span> In <em>Proceedings of the <span>Fourteenth International Conference</span> on <span>Artificial Intelligence</span> and <span>Statistics</span></em>, 752–60. <span>JMLR Workshop and Conference Proceedings</span>. <a href="https://proceedings.mlr.press/v15/wang11a.html">https://proceedings.mlr.press/v15/wang11a.html</a>.
</div>
<div id="ref-zeng2018" class="csl-entry" role="doc-biblioentry">
Zeng, Jianping, and Hongfeng Yu. 2018. <span>“A <span>Distributed Infomap Algorithm</span> for <span>Scalable</span> and <span>High</span>-<span>Quality Community Detection</span>.”</span> In <em>Proceedings of the 47th <span>International Conference</span> on <span>Parallel Processing</span></em>, 1–11. <span>ICPP</span> 2018. <span>New York, NY, USA</span>: <span>Association for Computing Machinery</span>. <a href="https://doi.org/10.1145/3225058.3225137">https://doi.org/10.1145/3225058.3225137</a>.
</div>
<div id="ref-zipf1965" class="csl-entry" role="doc-biblioentry">
Zipf, George K. 1965. <em>The <span>Psycho</span>-<span>Biology</span> of <span>Language</span>: An <span>Introdution</span> to <span>Dynamic Philology</span></em>. <span>Cambridge, MA, USA</span>: <span>MIT Press</span>.
</div>
</div>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>On the relative uselessness of too accurate representations there are two short essays by J.L. Borges (“Del rigor en la ciencia” in <em>“Historia universal de la infamia”</em>) and U. Eco (“Dell’impossibilità di costruire la carta dell’impero 1 a 1” in <em>“Il secondo diario minimo”</em>)<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Generally speaking we can use raw count or some kind of transformation like frequency (which is normalization in 1-norm) or TFIDF weights (see infra <a href="#sec:tfidf" data-reference-type="ref" data-reference="sec:tfidf">2.4</a>).<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>This notation is not formally correct since the indexes of C should belong to <span class="math inline">\(\mathbb{N}\)</span> and not to <span class="math inline">\(\Gamma\)</span> and <span class="math inline">\(\mathcal{W}\)</span>, but it is straightforward to define a bijection between a finite set and <span class="math inline">\(\mathbb{N}\)</span> by enumerating the elements of the set. In fact, calling <span class="math inline">\(e\)</span> the enumerating function, we should write <span class="math inline">\(C_{ e_\Gamma (d) e_{\mathcal{W}} (w) }\)</span>. In the rest of the thesis this bijections will be omitted to lighten the notation.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>The very same technique is also called Latent Semantic Index (LSI).<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>Many authors define LSA as <span class="math inline">\(X \approx U_K \Sigma V_K^T\)</span>, but it is not difficult to switch between the two definitions.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>In fact, being <span class="math inline">\(\lambda\)</span> an eigenvalue and <span class="math inline">\(v\)</span> its eigenvector, we have <span class="math inline">\(X^TXv=\lambda v \Rightarrow v^TX^TXv=\lambda v^Tv = \|Xv\|^2=\lambda \|v\|^2\)</span> and, since the square norm <span class="math inline">\(\|\cdot\|^2\)</span> for a <span class="math inline">\(\mathbb{R}\)</span>-valued vector is non negative, we prove that <span class="math inline">\(\lambda \geq 0\)</span>.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p>We have no theoretical reason to choose the positive value of the square root instead of the negative one, but an homogenous choice of the sign eliminates a source of ambiguity in the definition of <span class="math inline">\(\Sigma\)</span>.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><p>The Frobenius norm of a matrix <span class="math inline">\(A\)</span> is <span class="math inline">\(\|A\|_F=\sqrt{\sum_{ij}|a_{ij}|^2}\)</span> and if the matrix is square also hold <span class="math inline">\(\|A\|_F=\sqrt{\text{tr}(A^2)}\)</span><a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9" role="doc-endnote"><p>The cosine similarity between two vectors <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is defined as <span class="math inline">\(\rho(x,y) = \frac{x \cdot y}{\|x\|\|y\|}\)</span>, which is equal to the cosine of the angle between the vectors.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10" role="doc-endnote"><p>Often in literature the notation used is <span class="math inline">\(X \approx WH\)</span>, but we prefer to use <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> to avoid the ambiguity between the number of words and the matrix of the decomposition.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11" role="doc-endnote"><p>It is possible to obtain a factorization in two matrices with non-negative elements only for matrices with non-negative elements. Luckily the count matrix has non-negative elements.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12" role="doc-endnote"><p>The Kullback-Leiber divergence additionally requires that <span class="math inline">\(\sum_{ij}X_{ij}=\sum_{ij}(UV)_{ij}=1\)</span>.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13" role="doc-endnote"><p>As LSA is also known as LSI, PLSA is also called Probabilistic Latent Semantic Index (PLSI).<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14" role="doc-endnote"><p>This is the reason for introducing the Information Criteria to select a model in place of the Likelihood alone <span class="citation" data-cites="akaike1974">(e.g. <a href="#ref-akaike1974" role="doc-biblioref">Akaike 1974</a>)</span>.<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15" role="doc-endnote"><p>The Dirichlet distribution is the conjugate prior of the multinomial distribution, of which the categorical distribution is the special case with <span class="math inline">\(n=1\)</span>.<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16" role="doc-endnote"><p>Network theory and graph theory are very similar fields of research even if they were born in different research communities. But the two different lexicons are still both used: a graph is composed of vertexes and edges, while a network of nodes and links. For the practical use the terms are perfectly exchangeable, we have tried to not mix too much the two lexicons, preferring the network one.<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17" role="doc-endnote"><p><span id="fn:adj" label="fn:adj">[fn:adj]</span>A weighted undirected bipartite graph can be represented by a matrix in which to each vertex of the first group is assigned a row and to each vertex of the second group is assigned a column and the entries of the matrix are the weight. If we call <span class="math inline">\(C\)</span> this matrix we can obtain the full adjacency matrix, using the block notation, as <span class="math inline">\(A = \left(
    \begin{array}{cc}
        0 &amp; C \\ C^T &amp; 0
    \end{array}
\right)\)</span> where the vertexes are numbered following the order of the first partition and then continuing with the second.<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18" role="doc-endnote"><p>Sometimes in literature a graph whose edges are weighted and their weights are natural numbers which can be interpreted as counts, like in our case, is called multi-graph and the weighted edges are substitute with an equal number of unweighted edges between the same couple of nodes<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19" role="doc-endnote"><p>“<em>In order to effectively and concisely describe where on the network a random walker is, an effective encoding of position will necessarily exploit the regularities in patterns of movement on that network. If we can find an optimal code for describing places traced by a path on a network, we have also solved the dual problem of finding the important structural features of that network. Therefore, we look for a way to assign codewords to nodes that is efficient with respect to the dynamics on the network.</em>” <span class="citation" data-cites="rosvall2009">(<a href="#ref-rosvall2009" role="doc-biblioref">M. Rosvall, Axelsson, and Bergstrom 2009</a>)</span>, where the regularities are in fact the clusters of the network.<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20" role="doc-endnote"><p>The process used differs substantially from that illustrated in section <a href="#sec:plsa" data-reference-type="ref" data-reference="sec:plsa">4.1</a>, but still aims to get a better decomposition in the form <span class="math inline">\(\mathbb{P}(w|d) = \sum_k \mathbb{P}(k|d) \mathbb{P}(w|k)\)</span> than the one obtained with Infomap. The exact implementation is well explained in the supplementary materials of <span class="citation" data-cites="lancichinetti2015">Lancichinetti et al. (<a href="#ref-lancichinetti2015" role="doc-biblioref">2015</a>)</span>.<a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21" role="doc-endnote"><p>A parallel adaptation of InfoMap is available but it does not achieve the same accuracy and reproducibility <span class="citation" data-cites="zeng2018">(<a href="#ref-zeng2018" role="doc-biblioref">Zeng and Yu 2018</a>)</span>.<a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn22" role="doc-endnote"><p>In section <a href="#sec:plsa" data-reference-type="ref" data-reference="sec:plsa">4.1</a> we have defined the Kullback-Lieber divergence which has some of these characteristics but it is not symmetric for the exchange of the two distributions. It is suited to compare one distribution to another and not two distributions to themself.<a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn23" role="doc-endnote"><p><span class="math inline">\(\mathbb{P}(X \cap Y) = \mathbb{P}(X|Y)\mathbb{P}(Y)\)</span> and since <span class="math inline">\(\mathbb{P}(X|Y)\geq\mathbb{P}(X)\)</span> we have <span class="math inline">\(\mathbb{P}(X \cap Y)\geq\mathbb{P}(X)\mathbb{P}(Y)\)</span><a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn24" role="doc-endnote"><p><span class="math inline">\(\sum \mathbb{P}(\mathbf{X}\cap\mathbf{Y}) \log\frac{\mathbb{P}(\mathbf{X}\cap\mathbf{Y})}{\mathbb{P}(\mathbf{X})\mathbb{P}(\mathbf{Y})} 
    \leq \sum \mathbb{P}(\mathbf{X}) \log\frac{\mathbb{P}(\mathbf{X}\cap\mathbf{Y})}{\mathbb{P}(\mathbf{X})\mathbb{P}(\mathbf{Y})} 
    = - \sum \mathbb{P}(\mathbf{X}) \log\frac{\mathbb{P}(\mathbf{X})}{\mathbb{P}(\mathbf{X}|\mathbf{Y})}
    = - \sum \mathbb{P}(\mathbf{X}) \log \mathbb{P}(\mathbf{X}) + \sum \mathbb{P}(\mathbf{X}) \log {\mathbb{P}(\mathbf{X}|\mathbf{Y})}
    = H(\mathbf{X}) + \sum \mathbb{P}(\mathbf{X}) \log {\mathbb{P}(\mathbf{X}|\mathbf{Y})}
\leq H(\mathbf{X})\)</span> given <span class="math inline">\(\log {\mathbb{P}(\mathbf{X}|\mathbf{Y})} \leq 0\)</span>. The same holds for <span class="math inline">\(\mathbf{Y}\)</span> for symmetry.<a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn25" role="doc-endnote"><p><a href="http://qwone.com/~jason/20Newsgroups/">http://qwone.com/~jason/20Newsgroups/</a>, retrieved by using Scikit-Learn<a href="#fnref25" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn26" role="doc-endnote"><p><a href="https://data.mendeley.com/datasets/9rw3vkcfy4/6">https://data.mendeley.com/datasets/9rw3vkcfy4/6</a> <span class="citation" data-cites="kowsari2017">(<a href="#ref-kowsari2017" role="doc-biblioref">Kowsari et al. 2017</a>)</span><a href="#fnref26" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn27" role="doc-endnote"><p>A non exhaustive list includes: narrative texts from project Gutenberg labelled by their genre <span class="citation" data-cites="gerlachStandardizedProjectGutenberg2018">(<a href="#ref-gerlachStandardizedProjectGutenberg2018" role="doc-biblioref">Gerlach and Font-Clos 2018</a>)</span>, political manifestos where each sentence is classified by topic <span class="citation" data-cites="merzManifestoCorpusNew2016">(<a href="#ref-merzManifestoCorpusNew2016" role="doc-biblioref">Merz, Regel, and Lewandowski 2016</a>)</span>, Wikipedia articles with their categories, multilingual datasets with languages as labels<a href="#fnref27" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
