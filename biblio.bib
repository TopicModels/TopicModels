
@article{akaike1974,
  title        = {A New Look at the Statistical Model Identification},
  author       = {Akaike, H.},
  date         = {1974-12},
  journaltitle = {IEEE Trans. Automat. Contr.},
  volume       = {19},
  number       = {6},
  pages        = {716--723},
  issn         = {0018-9286},
  doi          = {10.1109/TAC.1974.1100705},
  url          = {http://ieeexplore.ieee.org/document/1100705/},
  urldate      = {2021-09-24},
  langid       = {english}
}

@article{ambrosino2018,
  title        = {What Topic Modeling Could Reveal about the Evolution of Economics},
  author       = {Ambrosino, Angela and Cedrini, Mario and Davis, John B. and Fiori, Stefano and Guerzoni, Marco and Nuccio, Massimiliano},
  date         = {2018-10-02},
  journaltitle = {Journal of Economic Methodology},
  volume       = {25},
  number       = {4},
  pages        = {329--348},
  issn         = {1350-178X, 1469-9427},
  doi          = {10/gf359h},
  url          = {https://www.tandfonline.com/doi/full/10.1080/1350178X.2018.1529215},
  urldate      = {2019-11-08},
  issue        = {4},
  langid       = {english},
  keywords     = {_tablet}
}

@online{angelov2020,
  title         = {{{Top2Vec}}: Distributed {{Representations}} of {{Topics}}},
  shorttitle    = {{{Top2Vec}}},
  author        = {Angelov, Dimo},
  date          = {2020-08-19},
  eprint        = {2008.09470},
  eprinttype    = {arxiv},
  primaryclass  = {cs, stat},
  url           = {http://arxiv.org/abs/2008.09470},
  urldate       = {2021-09-25},
  abstract      = {Topic modeling is used for discovering latent semantic structure, usually referred to as topics, in a large collection of documents. The most widely used methods are Latent Dirichlet Allocation and Probabilistic Latent Semantic Analysis. Despite their popularity they have several weaknesses. In order to achieve optimal results they often require the number of topics to be known, custom stop-word lists, stemming, and lemmatization. Additionally these methods rely on bag-of-words representation of documents which ignore the ordering and semantics of words. Distributed representations of documents and words have gained popularity due to their ability to capture semantics of words and documents. We present \$\textbackslash texttt\{top2vec\}\$, which leverages joint document and word semantic embedding to find \$\textbackslash textit\{topic vectors\}\$. This model does not require stop-word lists, stemming or lemmatization, and it automatically finds the number of topics. The resulting topic vectors are jointly embedded with the document and word vectors with distance between them representing semantic similarity. Our experiments demonstrate that \$\textbackslash texttt\{top2vec\}\$ finds topics which are significantly more informative and representative of the corpus trained on than probabilistic generative models.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{angrist2017,
  title        = {Economic {{Research Evolves}}: Fields and {{Styles}}},
  shorttitle   = {Economic {{Research Evolves}}},
  author       = {Angrist, Joshua and Azoulay, Pierre and Ellison, Glenn and Hill, Ryan and Lu, Susan Feng},
  date         = {2017-05},
  journaltitle = {American Economic Review},
  volume       = {107},
  number       = {5},
  pages        = {293--297},
  issn         = {0002-8282},
  doi          = {10/gdgx8x},
  url          = {http://pubs.aeaweb.org/doi/10.1257/aer.p20171117},
  urldate      = {2020-04-30},
  issue        = {5},
  langid       = {english},
  keywords     = {_tablet}
}


@online{arora2012,
  title         = {A {{Practical Algorithm}} for {{Topic Modeling}} with {{Provable Guarantees}}},
  author        = {Arora, Sanjeev and Ge, Rong and Halpern, Yoni and Mimno, David and Moitra, Ankur and Sontag, David and Wu, Yichen and Zhu, Michael},
  date          = {2012-12-19},
  eprint        = {1212.4777},
  eprinttype    = {arxiv},
  primaryclass  = {cs, stat},
  url           = {http://arxiv.org/abs/1212.4777},
  urldate       = {2020-11-10},
  abstract      = {Topic models provide a useful method for dimensionality reduction and exploratory data analysis in large text corpora. Most approaches to topic model inference have been based on a maximum likelihood objective. Efficient algorithms exist that approximate this objective, but they have no provable guarantees. Recently, algorithms have been introduced that provide provable bounds, but these algorithms are not practical because they are inefficient and not robust to violations of model assumptions. In this paper we present an algorithm for topic model inference that is both provable and practical. The algorithm produces results comparable to the best MCMC implementations while running orders of magnitude faster.},
  archiveprefix = {arXiv},
  langid        = {english},
  keywords      = {Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{ball2011,
  title        = {Efficient and Principled Method for Detecting Communities in Networks},
  author       = {Ball, Brian and Karrer, Brian and Newman, M. E. J.},
  date         = {2011-09-08},
  journaltitle = {Phys. Rev. E},
  volume       = {84},
  number       = {3},
  pages        = {036103},
  publisher    = {{American Physical Society}},
  doi          = {10.1103/PhysRevE.84.036103},
  url          = {https://link.aps.org/doi/10.1103/PhysRevE.84.036103},
  urldate      = {2021-09-25},
  abstract     = {A fundamental problem in the analysis of network data is the detection of network communities, groups of densely interconnected nodes, which may be overlapping or disjoint. Here we describe a method for finding overlapping communities based on a principled statistical approach using generative network models. We show how the method can be implemented using a fast, closed-form expectation-maximization algorithm that allows us to analyze networks of millions of nodes in reasonable running times. We test the method both on real-world networks and on synthetic benchmarks and find that it gives results competitive with previous methods. We also show that the same approach can be used to extract nonoverlapping community divisions via a relaxation method, and demonstrate that the algorithm is competitively fast and accurate for the nonoverlapping problem.}
}

@article{berry2007,
  title        = {Algorithms and Applications for Approximate Nonnegative Matrix Factorization},
  author       = {Berry, Michael W. and Browne, Murray and Langville, Amy N. and Pauca, V. Paul and Plemmons, Robert J.},
  date         = {2007-09},
  journaltitle = {Computational Statistics \& Data Analysis},
  volume       = {52},
  number       = {1},
  pages        = {155--173},
  issn         = {01679473},
  doi          = {10.1016/j.csda.2006.11.006},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0167947306004191},
  urldate      = {2020-11-11},
  abstract     = {In this paper we discuss the development and use of low-rank approximate nonnegative matrix factorization (NMF) algorithms for feature extraction and identification in the fields of text mining and spectral data analysis. The evolution and convergence properties of hybrid methods based on both sparsity and smoothness constraints for the resulting nonnegative matrix factors are discussed. The interpretability of NMF outputs in specific contexts are provided along with opportunities for future work in the modification of NMF algorithms for large-scale and time-varying datasets.},
  langid       = {english}
}

@article{blei2003,
  title        = {Latent Dirichlet Allocation},
  author       = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  date         = {2003-03-01},
  journaltitle = {J. Mach. Learn. Res.},
  volume       = {3},
  pages        = {993--1022},
  issn         = {1532-4435},
  abstract     = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
  issue        = {null}
}

@inproceedings{bleiDynamicTopicModels2006,
  title      = {Dynamic Topic Models},
  booktitle  = {Proceedings of the 23rd International Conference on {{Machine}} Learning  - {{ICML}} '06},
  author     = {Blei, David M. and Lafferty, John D.},
  date       = {2006},
  pages      = {113--120},
  publisher  = {{ACM Press}},
  location   = {{Pittsburgh, Pennsylvania}},
  doi        = {10/bjkmq9},
  url        = {http://portal.acm.org/citation.cfm?doid=1143844.1143859},
  urldate    = {2020-05-31},
  abstract   = {A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics. In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demonstrated by analyzing the OCR’ed archives of the journal Science from 1880 through 2000.},
  eventtitle = {The 23rd International Conference},
  isbn       = {978-1-59593-383-6},
  langid     = {english},
  annotation = {ZSCC: 0000000}
}

@article{blondel2008,
  title         = {Fast Unfolding of Communities in Large Networks},
  author        = {Blondel, Vincent D. and Guillaume, Jean-Loup and Lambiotte, Renaud and Lefebvre, Etienne},
  date          = {2008-10-09},
  journaltitle  = {J. Stat. Mech.},
  volume        = {2008},
  number        = {10},
  eprint        = {0803.0476},
  eprinttype    = {arxiv},
  pages         = {P10008},
  issn          = {1742-5468},
  doi           = {10.1088/1742-5468/2008/10/P10008},
  url           = {http://arxiv.org/abs/0803.0476},
  urldate       = {2020-05-18},
  abstract      = {We propose a simple method to extract the community structure of large networks. Our method is a heuristic method that is based on modularity optimization. It is shown to outperform all other known community detection method in terms of computation time. Moreover, the quality of the communities detected is very good, as measured by the so-called modularity. This is shown first by identifying language communities in a Belgian mobile phone network of 2.6 million customers and by analyzing a web graph of 118 million nodes and more than one billion links. The accuracy of our algorithm is also verified on ad-hoc modular networks.},
  archiveprefix = {arXiv},
  issue         = {10},
  langid        = {english},
  keywords      = {_tablet,Computer Science - Computers and Society,Computer Science - Data Structures and Algorithms,Condensed Matter - Statistical Mechanics,Physics - Physics and Society},
  annotation    = {ZSCC: 0000000}
}

@incollection{campello2013,
  title       = {Density-{{Based Clustering Based}} on {{Hierarchical Density Estimates}}},
  booktitle   = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  author      = {Campello, Ricardo J. G. B. and Moulavi, Davoud and Sander, Joerg},
  editor      = {Pei, Jian and Tseng, Vincent S. and Cao, Longbing and Motoda, Hiroshi and Xu, Guandong},
  date        = {2013},
  series      = {Lecture {{Notes}} in {{Computer Science}}},
  volume      = {7819},
  pages       = {160--172},
  publisher   = {{Springer Berlin Heidelberg}},
  location    = {{Berlin, Heidelberg}},
  doi         = {10.1007/978-3-642-37456-2_14},
  url         = {http://link.springer.com/10.1007/978-3-642-37456-2_14},
  urldate     = {2021-09-25},
  editorb     = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
  editorbtype = {redactor},
  isbn        = {978-3-642-37455-5 978-3-642-37456-2}
}

@article{deerwester1990,
  title        = {Indexing by Latent Semantic Analysis},
  author       = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
  date         = {1990},
  journaltitle = {Journal of the American Society for Information Science},
  volume       = {41},
  number       = {6},
  pages        = {391--407},
  issn         = {1097-4571},
  doi          = {10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9},
  url          = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9},
  urldate      = {2021-08-09},
  abstract     = {A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising. © 1990 John Wiley \& Sons, Inc.},
  langid       = {english},
  annotation   = {\_eprint: https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/\%28SICI\%291097-4571\%28199009\%2941\%3A6\%3C391\%3A\%3AAID-ASI1\%3E3.0.CO\%3B2-9}
}

@online{dicaroBimodalNetworkApproach2017,
  title         = {A {{Bimodal Network Approach}} to {{Model Topic Dynamics}}},
  author        = {Di Caro, Luigi and Guerzoni, Marco and Nuccio, Massimiliano and Siragusa, Giovanni},
  date          = {2017-09-27},
  eprint        = {1709.09373},
  eprinttype    = {arxiv},
  primaryclass  = {cs, econ, q-fin},
  url           = {http://arxiv.org/abs/1709.09373},
  urldate       = {2020-04-30},
  abstract      = {This paper presents an intertemporal bimodal network to analyze the evolution of the semantic content of a scientific field within the framework of topic modeling, namely using the Latent Dirichlet Allocation (LDA). The main contribution is the conceptualization of the topic dynamics and its formalization and codification into an algorithm. To benchmark the effectiveness of this approach, we propose three indexes which track the transformation of topics over time, their rate of birth and death, and the novelty of their content. Applying the LDA, we test the algorithm both on a controlled experiment and on a corpus of several thousands of scientific papers over a period of more than 100 years which account for the history of the economic thought.},
  archiveprefix = {arXiv},
  langid        = {english},
  keywords      = {_tablet,⛔ No DOI found,Computer Science - Computation and Language,Economics - General Economics}
}

@incollection{ding2005,
  title     = {On the {{Equivalence}} of {{Nonnegative Matrix Factorization}} and {{Spectral Clustering}}},
  booktitle = {Proceedings of the 2005 {{SIAM International Conference}} on {{Data Mining}} ({{SDM}})},
  author    = {Ding, Chris and He, Xiaofeng and Simon, Horst D.},
  date      = {2005-04-21},
  series    = {Proceedings},
  pages     = {606--610},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi       = {10.1137/1.9781611972757.70},
  url       = {https://epubs.siam.org/doi/abs/10.1137/1.9781611972757.70},
  urldate   = {2021-09-23},
  abstract  = {Current nonnegative matrix factorization (NMF) deals with X = FGT type. We provide a systematic analysis and extensions of NMF to the symmetric W = HHT, and the weighted W = HSHT. We show that (1) W = HHT is equivalent to Kernel if-means clustering and the Laplacian-based spectral clustering. (2) X = FGT is equivalent to simultaneous clustering of rows and columns of a bipartite graph. Algorithms are given for computing these symmetric NMFs.},
  isbn      = {978-0-89871-593-4}
}

@article{eckart1936,
  title        = {The Approximation of One Matrix by Another of Lower Rank},
  author       = {Eckart, Carl and Young, Gale},
  date         = {1936-09-01},
  journaltitle = {Psychometrika},
  volume       = {1},
  number       = {3},
  pages        = {211--218},
  issn         = {1860-0980},
  doi          = {10.1007/BF02288367},
  url          = {https://doi.org/10.1007/BF02288367},
  urldate      = {2021-08-10},
  abstract     = {The mathematical problem of approximating one matrix by another of lower rank is closely related to the fundamental postulate of factor-theory. When formulated as a least-squares problem, the normal equations cannot be immediately written down, since the elements of the approximate matrix are not independent of one another. The solution of the problem is simplified by first expressing the matrices in a canonic form. It is found that the problem always has a solution which is usually unique. Several conclusions can be drawn from the form of this solution.},
  langid       = {english}
}

@inproceedings{gaussier2005,
  title      = {Relation between {{PLSA}} and {{NMF}} and Implications},
  booktitle  = {Proceedings of the 28th Annual International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval - {{SIGIR}} '05},
  author     = {Gaussier, Eric and Goutte, Cyril},
  date       = {2005},
  pages      = {601},
  publisher  = {{ACM Press}},
  location   = {{Salvador, Brazil}},
  doi        = {10.1145/1076034.1076148},
  url        = {http://dl.acm.org/citation.cfm?doid=1076034.1076148},
  urldate    = {2020-11-10},
  abstract   = {Non-negative Matrix Factorization (NMF, [5]) and Probabilistic Latent Semantic Analysis (PLSA, [4]) have been successfully applied to a number of text analysis tasks such as document clustering. Despite their different inspirations, both methods are instances of multinomial PCA [1]. We further explore this relationship and first show that PLSA solves the problem of NMF with KL divergence, and then explore the implications of this relationship.},
  eventtitle = {The 28th Annual International {{ACM SIGIR}} Conference},
  isbn       = {978-1-59593-034-7},
  langid     = {english}
}

@article{gerlach2018,
  title         = {A Network Approach to Topic Models},
  author        = {Gerlach, Martin and Peixoto, Tiago P. and Altmann, Eduardo G.},
  date          = {2018-07},
  journaltitle  = {Sci. Adv.},
  volume        = {4},
  number        = {7},
  eprint        = {1708.01677},
  eprinttype    = {arxiv},
  pages         = {eaaq1360},
  issn          = {2375-2548},
  doi           = {10/gdxnxq},
  url           = {http://arxiv.org/abs/1708.01677},
  urldate       = {2020-02-27},
  abstract      = {One of the main computational and scientific challenges in the modern age is to extract useful information from unstructured texts. Topic models are one popular machine-learning approach which infers the latent topical structure of a collection of documents. Despite their success --- in particular of its most widely used variant called Latent Dirichlet Allocation (LDA) --- and numerous applications in sociology, history, and linguistics, topic models are known to suffer from severe conceptual and practical problems, e.g. a lack of justification for the Bayesian priors, discrepancies with statistical properties of real texts, and the inability to properly choose the number of topics. Here we obtain a fresh view on the problem of identifying topical structures by relating it to the problem of finding communities in complex networks. This is achieved by representing text corpora as bipartite networks of documents and words. By adapting existing community-detection methods -- using a stochastic block model (SBM) with non-parametric priors -- we obtain a more versatile and principled framework for topic modeling (e.g., it automatically detects the number of topics and hierarchically clusters both the words and documents). The analysis of artificial and real corpora demonstrates that our SBM approach leads to better topic models than LDA in terms of statistical model selection. More importantly, our work shows how to formally relate methods from community detection and topic modeling, opening the possibility of cross-fertilization between these two fields.},
  archiveprefix = {arXiv},
  issue         = {7},
  langid        = {english},
  keywords      = {_tablet,Computer Science - Computation and Language,Physics - Data Analysis; Statistics and Probability,Physics - Physics and Society,Statistics - Machine Learning},
  annotation    = {ZSCC: 0000000}
}

@article{gerlach2019,
  title        = {A Universal Information Theoretic Approach to the Identification of Stopwords},
  author       = {Gerlach, Martin and Shi, Hanyu and Amaral, Luís A. Nunes},
  date         = {2019-12},
  journaltitle = {Nat Mach Intell},
  volume       = {1},
  number       = {12},
  pages        = {606--612},
  issn         = {2522-5839},
  doi          = {10.1038/s42256-019-0112-6},
  url          = {http://www.nature.com/articles/s42256-019-0112-6},
  urldate      = {2020-11-06},
  langid       = {english}
}

@online{gerlachStandardizedProjectGutenberg2018,
  title         = {A Standardized {{Project Gutenberg}} Corpus for Statistical Analysis of Natural Language and Quantitative Linguistics},
  author        = {Gerlach, Martin and Font-Clos, Francesc},
  date          = {2018-12-19},
  eprint        = {1812.08092},
  eprinttype    = {arxiv},
  primaryclass  = {physics},
  url           = {http://arxiv.org/abs/1812.08092},
  urldate       = {2020-02-27},
  abstract      = {The use of Project Gutenberg (PG) as a text corpus has been extremely popular in statistical analysis of language for more than 25 years. However, in contrast to other major linguistic datasets of similar importance, no consensual full version of PG exists to date. In fact, most PG studies so far either consider only a small number of manually selected books, leading to potential biased subsets, or employ vastly different pre-processing strategies (often specified in insufficient details), raising concerns regarding the reproducibility of published results. In order to address these shortcomings, here we present the Standardized Project Gutenberg Corpus (SPGC), an open science approach to a curated version of the complete PG data containing more than 50,000 books and more than \$3 \textbackslash times 10\^9\$ word-tokens. Using different sources of annotated metadata, we not only provide a broad characterization of the content of PG, but also show different examples highlighting the potential of SPGC for investigating language variability across time, subjects, and authors. We publish our methodology in detail, the code to download and process the data, as well as the obtained corpus itself on 3 different levels of granularity (raw text, timeseries of word tokens, and counts of words). In this way, we provide a reproducible, pre-processed, full-size version of Project Gutenberg as a new scientific resource for corpus linguistics, natural language processing, and information retrieval.},
  archiveprefix = {arXiv},
  langid        = {english},
  keywords      = {_tablet,⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Digital Libraries,Computer Science - Information Retrieval,Physics - Physics and Society},
  annotation    = {ZSCC: NoCitationData[s0]}
}

@article{griffiths2004,
  title        = {Finding Scientific Topics},
  author       = {Griffiths, T. L. and Steyvers, M.},
  date         = {2004-04-06},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume       = {101},
  pages        = {5228--5235},
  issn         = {0027-8424, 1091-6490},
  doi          = {10.1073/pnas.0307752101},
  url          = {http://www.pnas.org/cgi/doi/10.1073/pnas.0307752101},
  urldate      = {2020-11-28},
  issue        = {Supplement 1},
  langid       = {english}
}

@software{grootendorst2021,
  title        = {{{MaartenGr}}/{{BERTopic}}: V0.9.1},
  shorttitle   = {{{MaartenGr}}/{{BERTopic}}},
  author       = {Grootendorst, Maarten and Reimers, Nils},
  date         = {2021-09-01},
  doi          = {10.5281/ZENODO.4381785},
  url          = {https://zenodo.org/record/4381785},
  urldate      = {2021-09-25},
  abstract     = {{$<$}strong{$>$}Fixes{$<$}/strong{$>$}: Fix TypeError when auto-reducing topics (\#210) Fix mapping representative docs when reducing topics (\#208) Fix visualization issues with probabilities (\#205) Fix missing {$<$}code{$>$}normalize\_frequency{$<$}/code{$>$} param in plots (\#213)},
  organization = {{Zenodo}},
  version      = {v0.9.1}
}

@article{heck2006,
  title        = {The Most Frequent Contributors to the Elite Economics Journals: Half Century of Contributions to the “{{Blue}} Ribbon Eight”},
  shorttitle   = {The Most Frequent Contributors to the Elite Economics Journals},
  author       = {Heck, Jean L. and Zaleski, Peter A.},
  date         = {2006-03},
  journaltitle = {J Econ Finan},
  volume       = {30},
  number       = {1},
  pages        = {1--37},
  issn         = {1055-0925, 1938-9744},
  doi          = {10/drbxd5},
  url          = {http://link.springer.com/10.1007/BF02834273},
  urldate      = {2020-04-30},
  issue        = {1},
  langid       = {english},
  keywords     = {_tablet},
  annotation   = {ZSCC: 0000000}
}

@article{heckman2020,
  title        = {Publishing and {{Promotion}} in {{Economics}}: The {{Tyranny}} of the {{Top Five}}},
  shorttitle   = {Publishing and {{Promotion}} in {{Economics}}},
  author       = {Heckman, James J. and Moktan, Sidharth},
  date         = {2020-06-01},
  journaltitle = {Journal of Economic Literature},
  volume       = {58},
  number       = {2},
  pages        = {419--470},
  issn         = {0022-0515},
  doi          = {10/ggznwd},
  url          = {https://pubs.aeaweb.org/doi/10.1257/jel.20191574},
  urldate      = {2020-06-08},
  abstract     = {This paper examines the relationship between placement of publications in top five (T5) journals and receipt of tenure in academic economics departments. Analyzing the job histories of tenure–track economists hired by the top 35 US economics departments, we find that T5 publications have a powerful influence on tenure decisions and rates of transition to tenure. A survey of the perceptions of young economists supports the formal statistical analysis. Pursuit of T5 publications has become the obsession of the next generation of economists. However, the T5 screen is far from reliable. A substantial share of influential publications appear in non-T5 outlets. Reliance on the T5 to screen talent incentivizes careerism over creativity.( JEL A14, I23, J44, J62)},
  issue        = {2},
  langid       = {english},
  annotation   = {ZSCC: 0000000}
}

@inproceedings{hoffman2010,
  title     = {Online {{Learning}} for {{Latent Dirichlet Allocation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author    = {Hoffman, Matthew and Bach, Francis and Blei, David},
  date      = {2010},
  volume    = {23},
  publisher = {{Curran Associates, Inc.}},
  url       = {https://papers.nips.cc/paper/2010/hash/71f6278d140af599e06ad9bf1ba03cb0-Abstract.html},
  urldate   = {2021-09-24}
}

@online{hofmann1999,
  title         = {Probabilistic {{Latent Semantic Analysis}}},
  author        = {Hofmann, Thomas},
  date          = {1999},
  eprint        = {1301.6705},
  eprinttype    = {arxiv},
  primaryclass  = {cs, stat},
  url           = {http://arxiv.org/abs/1301.6705},
  urldate       = {2020-06-02},
  abstract      = {Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two-mode and co-occurrence data, which has applications in information retrieval and filtering, natural language processing, machine learning from text, and in related areas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. This results in a more principled approach which has a solid foundation in statistics. In order to avoid overfitting, we propose a widely applicable generalization of maximum likelihood model fitting by tempered EM. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of experiments.},
  archiveprefix = {arXiv},
  keywords      = {⛔ No DOI found,Computer Science - Information Retrieval,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation    = {ZSCC: 0000000}
}

@inproceedings{hsieh2011,
  title      = {Fast Coordinate Descent Methods with Variable Selection for Non-Negative Matrix Factorization},
  booktitle  = {Proceedings of the 17th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining - {{KDD}} '11},
  author     = {Hsieh, Cho-Jui and Dhillon, Inderjit S.},
  date       = {2011},
  pages      = {1064},
  publisher  = {{ACM Press}},
  location   = {{San Diego, California, USA}},
  doi        = {10.1145/2020408.2020577},
  url        = {http://dl.acm.org/citation.cfm?doid=2020408.2020577},
  urldate    = {2021-08-17},
  abstract   = {Nonnegative Matrix Factorization (NMF) is an effective dimension reduction method for non-negative dyadic data, and has proven to be useful in many areas, such as text mining, bioinformatics and image processing. NMF is usually formulated as a constrained non-convex optimization problem, and many algorithms have been developed for solving it. Recently, a coordinate descent method, called FastHals [3], has been proposed to solve least squares NMF and is regarded as one of the state-of-the-art techniques for the problem. In this paper, we first show that FastHals has an inefficiency in that it uses a cyclic coordinate descent scheme and thus, performs unneeded descent steps on unimportant variables. We then present a variable selection scheme that uses the gradient of the objective function to arrive at a new coordinate descent method. Our new method is considerably faster in practice and we show that it has theoretical convergence guarantees. Moreover when the solution is sparse, as is often the case in real applications, our new method benefits by selecting important variables to update more often, thus resulting in higher speed. As an example, on a text dataset RCV1, our method is 7 times faster than FastHals, and more than 15 times faster when the sparsity is increased by adding an L1 penalty. We also develop new coordinate descent methods when error in NMF is measured by KLdivergence by applying the Newton method to solve the one-variable sub-problems. Experiments indicate that our algorithm for minimizing the KL-divergence is faster than the Lee \& Seung multiplicative rule by a factor of 10 on the CBCL image dataset.},
  eventtitle = {The 17th {{ACM SIGKDD}} International Conference},
  isbn       = {978-1-4503-0813-7},
  langid     = {english}
}

@article{hyland2021,
  title        = {Multilayer Networks for Text Analysis with Multiple Data Types},
  author       = {Hyland, Charles C. and Tao, Yuanming and Azizi, Lamiae and Gerlach, Martin and Peixoto, Tiago P. and Altmann, Eduardo G.},
  date         = {2021-12},
  journaltitle = {EPJ Data Sci.},
  volume       = {10},
  number       = {1},
  pages        = {33},
  issn         = {2193-1127},
  doi          = {10.1140/epjds/s13688-021-00288-5},
  url          = {https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-021-00288-5},
  urldate      = {2021-09-25},
  abstract     = {Abstract             We are interested in the widespread problem of clustering documents and finding topics in large collections of written documents in the presence of metadata and hyperlinks. To tackle the challenge of accounting for these different types of datasets, we propose a novel framework based on Multilayer Networks and Stochastic Block Models. The main innovation of our approach over other techniques is that it applies the same non-parametric probabilistic framework to the different sources of datasets simultaneously. The key difference to other multilayer complex networks is the strong unbalance between the layers, with the average degree of different node types scaling differently with system size. We show that the latter observation is due to generic properties of text, such as Heaps’ law, and strongly affects the inference of communities. We present and discuss the performance of our method in different datasets (hundreds of Wikipedia documents, thousands of scientific papers, and thousands of E-mails) showing that taking into account multiple types of information provides a more nuanced view on topic- and document-clusters and increases the ability to predict missing links.},
  langid       = {english}
}

@article{jelodar2019,
  title        = {Latent {{Dirichlet}} Allocation ({{LDA}}) and Topic Modeling: Models, Applications, a Survey},
  shorttitle   = {Latent {{Dirichlet}} Allocation ({{LDA}}) and Topic Modeling},
  author       = {Jelodar, Hamed and Wang, Yongli and Yuan, Chi and Feng, Xia and Jiang, Xiahui and Li, Yanchao and Zhao, Liang},
  date         = {2019-06},
  journaltitle = {Multimed Tools Appl},
  volume       = {78},
  number       = {11},
  pages        = {15169--15211},
  issn         = {1380-7501, 1573-7721},
  doi          = {10.1007/s11042-018-6894-4},
  url          = {http://link.springer.com/10.1007/s11042-018-6894-4},
  urldate      = {2021-01-21},
  abstract     = {Topic modeling is one of the most powerful techniques in text mining for data mining, latent data discovery, and finding relationships among data and text documents. Researchers have published many articles in the field of topic modeling and applied in various fields such as software engineering, political science, medical and linguistic science, etc. There are various methods for topic modelling; Latent Dirichlet Allocation (LDA) is one of the most popular in this field. Researchers have proposed various models based on the LDA in topic modeling. According to previous work, this paper will be very useful and valuable for introducing LDA approaches in topic modeling. In this paper, we investigated highly scholarly articles (between 2003 to 2016) related to topic modeling based on LDA to discover the research development, current trends and intellectual structure of topic modeling. In addition, we summarize challenges and introduce famous tools and datasets in topic modeling based on LDA.},
  langid       = {english}
}

@article{kang2018,
  title        = {Opinion Mining Using Ensemble Text Hidden {{Markov}} Models for Text Classification},
  author       = {Kang, Mangi and Ahn, Jaelim and Lee, Kichun},
  date         = {2018-03},
  journaltitle = {Expert Systems with Applications},
  volume       = {94},
  pages        = {218--227},
  issn         = {09574174},
  doi          = {10.1016/j.eswa.2017.07.019},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0957417417304979},
  urldate      = {2021-07-18},
  langid       = {english}
}

@inproceedings{kido2016,
  title      = {Topic {{Modeling}} Based on {{Louvain}} Method in {{Online Social Networks}}},
  booktitle  = {Anais Do {{Simpósio Brasileiro}} de {{Sistemas}} de {{Informação}} ({{SBSI}})},
  author     = {Kido, Guilherme Sakaji and Igawa, Rodrigo Augusto and Barbon Jr., Sylvio},
  date       = {2016-05-17},
  pages      = {353--360},
  publisher  = {{Sociedade Brasileira de Computação}},
  doi        = {10.5753/sbsi.2016.5982},
  url        = {https://sol.sbc.org.br/index.php/sbsi/article/view/5982},
  urldate    = {2020-11-27},
  abstract   = {Online Social Networks (OSNs) are the most used media nowadays, such as Twitter. The OSNs provide valuable information to marketing and competitiveness based on users posts and opinions stored inside huge volume of data from several themes, topics and subjects. In order to mining the topics discussed on an OSN we present a novel application of Louvain method for Topic Modeling based on communities detection in graphs by modularity. The proposed approach succeeded in finding topics in five different datasets composed of textual content from Twitter and Youtube. Another important contribution achieved was about the presence of texts posted by spammers. In this case, a particular behavior observed by graph architecture (density and degree) allows the classification of a topic as natural or artificial, this last created by the spammers on OSNs.},
  eventtitle = {{{XII Simpósio Brasileiro}} de {{Sistemas}} de {{Informação}}},
  langid     = {english}
}

@report{kim2008,
  type        = {Technical Report},
  title       = {Sparse {{Nonnegative Matrix Factorization}} for {{Clustering}}},
  author      = {Kim, Jingu and Park, Haesun},
  date        = {2008},
  institution = {{Georgia Institute of Technology}},
  url         = {https://smartech.gatech.edu/handle/1853/20058},
  urldate     = {2021-09-29},
  abstract    = {Properties of Nonnegative Matrix Factorization (NMF) as a clustering method are studied by relating
                 its formulation to other methods such as K-means clustering. We show how interpreting the objective
                 function of K-means as that of a lower rank approximation with special constraints allows comparisons
                 between the constraints of NMF and K-means and provides the insight that some constraints can be
                 relaxed from K-means to achieve NMF formulation. By introducing sparsity constraints on the coefficient
                 matrix factor in NMF objective function, we in term can view NMF as a clustering method. We tested
                 sparse NMF as a clustering method, and our experimental results with synthetic and text data shows
                 that sparse NMF does not simply provide an alternative to K-means, but rather gives much better and
                 consistent solutions to the clustering problem. In addition, the consistency of solutions further explains
                 how NMF can be used to determine the unknown number of clusters from data. We also tested with a
                 recently proposed clustering algorithm, Affinity Propagation, and achieved comparable results. A fast
                 alternating nonnegative least squares algorithm was used to obtain NMF and sparse NMF.},
  langid      = {american},
  annotation  = {Accepted: 2008-02-21T22:54:42Z}
}

@article{kowsari2017,
  title         = {{{HDLTex}}: Hierarchical {{Deep Learning}} for {{Text Classification}}},
  shorttitle    = {{{HDLTex}}},
  author        = {Kowsari, Kamran and Brown, Donald E. and Heidarysafa, Mojtaba and Meimandi, Kiana Jafari and Gerber, Matthew S. and Barnes, Laura E.},
  date          = {2017-12},
  journaltitle  = {2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)},
  eprint        = {1709.08267},
  eprinttype    = {arxiv},
  pages         = {364--371},
  doi           = {10.1109/ICMLA.2017.0-134},
  url           = {http://arxiv.org/abs/1709.08267},
  urldate       = {2021-10-06},
  abstract      = {The continually increasing number of documents produced each year necessitates ever improving information processing methods for searching, retrieving, and organizing text. Central to these information processing methods is document classification, which has become an important application for supervised learning. Recently the performance of these traditional classifiers has degraded as the number of documents has increased. This is because along with this growth in the number of documents has come an increase in the number of categories. This paper approaches this problem differently from current document classification methods that view the problem as multi-class classification. Instead we perform hierarchical classification using an approach we call Hierarchical Deep Learning for Text classification (HDLTex). HDLTex employs stacks of deep learning architectures to provide specialized understanding at each level of the document hierarchy.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval,Computer Science - Machine Learning}
}

@article{lancichinetti2015,
  title        = {High-{{Reproducibility}} and {{High}}-{{Accuracy Method}} for {{Automated Topic Classification}}},
  author       = {Lancichinetti, Andrea and Sirer, M. Irmak and Wang, Jane X. and Acuna, Daniel and Körding, Konrad and Amaral, Luís A. Nunes},
  date         = {2015-01-29},
  journaltitle = {Phys. Rev. X},
  volume       = {5},
  number       = {1},
  pages        = {011007},
  issn         = {2160-3308},
  doi          = {10/gfrbjc},
  url          = {https://link.aps.org/doi/10.1103/PhysRevX.5.011007},
  urldate      = {2020-02-27},
  issue        = {1},
  langid       = {english},
  keywords     = {_tablet},
  annotation   = {ZSCC: 0000000}
}

@online{langville2014,
  title         = {Algorithms, {{Initializations}}, and {{Convergence}} for the {{Nonnegative Matrix Factorization}}},
  author        = {Langville, Amy N. and Meyer, Carl D. and Albright, Russell and Cox, James and Duling, David},
  date          = {2014-07-27},
  eprint        = {1407.7299},
  eprinttype    = {arxiv},
  primaryclass  = {cs, stat},
  url           = {http://arxiv.org/abs/1407.7299},
  urldate       = {2021-08-17},
  abstract      = {It is well known that good initializations can improve the speed and accuracy of the solutions of many nonnegative matrix factorization (NMF) algorithms. Many NMF algorithms are sensitive with respect to the initialization of W or H or both. This is especially true of algorithms of the alternating least squares (ALS) type, including the two new ALS algorithms that we present in this paper. We compare the results of six initialization procedures (two standard and four new) on our ALS algorithms. Lastly, we discuss the practical issue of choosing an appropriate convergence criterion.},
  archiveprefix = {arXiv},
  keywords      = {65F30,Computer Science - Machine Learning,Mathematics - Numerical Analysis,Statistics - Machine Learning}
}

@inproceedings{lee2000,
  title     = {Algorithms for Non-Negative Matrix Factorization},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{Neural Information Processing Systems}}},
  author    = {Lee, Daniel D. and Seung, H. Sebastian},
  date      = {2000-01-01},
  series    = {{{NIPS}}'00},
  pages     = {535--541},
  publisher = {{MIT Press}},
  location  = {{Cambridge, MA, USA}},
  abstract  = {Non-negative matrix factorization (NMF) has previously been shown to be a useful decomposition for multivariate data. Two different multiplicative algorithms for NMF are analyzed. They differ only slightly in the multiplicative factor used in the update rules. One algorithm can be shown to minimize the conventional least squares error while the other minimizes the generalized Kullback-Leibler divergence. The monotonic convergence of both algorithms can be proven using an auxiliary function analogous to that used for proving convergence of the Expectation-Maximization algorithm. The algorithms can also be interpreted as diagonally rescaled gradient descent, where the rescaling factor is optimally chosen to ensure convergence.}
}

@inproceedings{lei2014,
  title      = {Generalized Information Theoretic Cluster Validity Indices for Soft Clusterings},
  booktitle  = {2014 {{IEEE Symposium}} on {{Computational Intelligence}} and {{Data Mining}} ({{CIDM}})},
  author     = {Lei, Yang and Bezdek, James C. and Chan, Jeffrey and Xuan Vinh, Nguyen and Romano, Simone and Bailey, James},
  date       = {2014-12},
  pages      = {24--31},
  publisher  = {{IEEE}},
  location   = {{Orlando, FL, USA}},
  doi        = {10.1109/CIDM.2014.7008144},
  url        = {http://ieeexplore.ieee.org/document/7008144/},
  urldate    = {2021-04-28},
  eventtitle = {2014 {{IEEE Symposium}} on {{Computational Intelligence}} and {{Data Mining}} ({{CIDM}})},
  isbn       = {978-1-4799-4518-4}
}


@article{mccallum,
  title    = {Rethinking {{LDA}}: Why {{Priors Matter}}},
  author   = {McCallum, Andrew and Mimno, David M and Wallach, Hanna M},
  pages    = {9},
  abstract = {Implementations of topic models typically use symmetric Dirichlet priors with fixed concentration parameters, with the implicit assumption that such “smoothing parameters” have little practical effect. In this paper, we explore several classes of structured priors for topic models. We find that an asymmetric Dirichlet prior over the document–topic distributions has substantial advantages over a symmetric prior, while an asymmetric prior over the topic–word distributions provides no real benefit. Approximation of this prior structure through simple, efficient hyperparameter optimization steps is sufficient to achieve these performance gains. The prior structure we advocate substantially increases the robustness of topic models to variations in the number of topics and to the highly skewed word frequency distributions common in natural language. Since this prior structure can be implemented using efficient algorithms that add negligible cost beyond standard inference techniques, we recommend it as a new standard for topic modeling.},
  langid   = {english}
}

@unpublished{McCallumMALLET,
  title  = {{{MALLET}}: A Machine Learning for Language Toolkit},
  author = {McCallum, Andrew Kachites},
  date   = {2002}
}

@online{mcinnes2020,
  title         = {{{UMAP}}: Uniform {{Manifold Approximation}} and {{Projection}} for {{Dimension Reduction}}},
  shorttitle    = {{{UMAP}}},
  author        = {McInnes, Leland and Healy, John and Melville, James},
  date          = {2020-09-17},
  eprint        = {1802.03426},
  eprinttype    = {arxiv},
  primaryclass  = {cs, stat},
  url           = {http://arxiv.org/abs/1802.03426},
  urldate       = {2021-09-25},
  abstract      = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{merzManifestoCorpusNew2016,
  title        = {The {{Manifesto Corpus}}: A New Resource for Research on Political Parties and Quantitative Text Analysis},
  shorttitle   = {The {{Manifesto Corpus}}},
  author       = {Merz, Nicolas and Regel, Sven and Lewandowski, Jirka},
  date         = {2016-04-19},
  journaltitle = {Research \& Politics},
  volume       = {3},
  number       = {2},
  pages        = {205316801664334},
  issn         = {2053-1680, 2053-1680},
  doi          = {10/gftn2f},
  url          = {http://journals.sagepub.com/doi/10.1177/2053168016643346},
  urldate      = {2020-05-31},
  abstract     = {This article presents a digital, open-access, multilingual, annotated corpus of electoral programs. It complements the recent methodological innovations in (semi-) computerized content analysis by providing a large, standardized text corpus for the political science community. The corpus is based on the collection of the Manifesto Project, which comprises of (at the time of writing) the largest hand-annotated text corpus of electoral programs available. Since 2009 the project’s costly and time-intensive procedure of collecting and coding documents has been fully digitized. As a result, it now provides more than 1800 machine readable documents from 40 different countries. Six hundred of these documents contain content-analyzed annotations at the level of single (quasi-) sentences, which correspond to the Manifesto Project coding scheme. Additionally, the corpus will continually be extended by incorporating new elections and digitizing older documents. The database also provides meta-information for each document (eg. party, election, language, etc.) that allow it to be referenced back to the Manifesto Dataset. The corpus is stored in a standardized format in an online database, and an API and R package (manifestoR) guarantee easy access.},
  issue        = {2},
  langid       = {english}
}

@online{mikolov2013,
  title         = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author        = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  date          = {2013-09-06},
  eprint        = {1301.3781},
  eprinttype    = {arxiv},
  primaryclass  = {cs},
  url           = {http://arxiv.org/abs/1301.3781},
  urldate       = {2021-09-29},
  abstract      = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computation and Language}
}

@article{miller1995,
  title        = {{{WordNet}}: A Lexical Database for {{English}}},
  shorttitle   = {{{WordNet}}},
  author       = {Miller, George A.},
  date         = {1995-11},
  journaltitle = {Commun. ACM},
  volume       = {38},
  number       = {11},
  pages        = {39--41},
  issn         = {0001-0782, 1557-7317},
  doi          = {10.1145/219717.219748},
  url          = {https://dl.acm.org/doi/10.1145/219717.219748},
  urldate      = {2021-07-19},
  abstract     = {Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet               1               provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].},
  langid       = {english}
}

@article{newman2009,
  title        = {Distributed {{Algorithms}} for {{Topic Models}}},
  author       = {Newman, David and Asuncion, Arthur and Smyth, Padhraic and Welling, Max},
  date         = {2009},
  journaltitle = {Journal of Machine Learning Research},
  volume       = {10},
  number       = {62},
  pages        = {1801--1828},
  issn         = {1533-7928},
  url          = {http://jmlr.org/papers/v10/newman09a.html},
  urldate      = {2021-09-24},
  langid       = {english}
}

@online{peixoto2019,
  title         = {Bayesian Stochastic Blockmodeling},
  author        = {Peixoto, Tiago P.},
  date          = {2019-11-23},
  eprint        = {1705.10225},
  eprinttype    = {arxiv},
  primaryclass  = {cond-mat, physics:physics, stat},
  pages         = {289--332},
  doi           = {10.1002/9781119483298.ch11},
  url           = {http://arxiv.org/abs/1705.10225},
  urldate       = {2021-09-25},
  abstract      = {This chapter provides a self-contained introduction to the use of Bayesian inference to extract large-scale modular structures from network data, based on the stochastic blockmodel (SBM), as well as its degree-corrected and overlapping generalizations. We focus on nonparametric formulations that allow their inference in a manner that prevents overfitting, and enables model selection. We discuss aspects of the choice of priors, in particular how to avoid underfitting via increased Bayesian hierarchies, and we contrast the task of sampling network partitions from the posterior distribution with finding the single point estimate that maximizes it, while describing efficient algorithms to perform either one. We also show how inferring the SBM can be used to predict missing and spurious links, and shed light on the fundamental limitations of the detectability of modular structures in networks.},
  archiveprefix = {arXiv},
  keywords      = {Condensed Matter - Statistical Mechanics,Physics - Data Analysis; Statistics and Probability,Statistics - Machine Learning}
}

@article{porter1980,
  title        = {An Algorithm for Suffix Stripping},
  author       = {Porter, M.F.},
  date         = {1980-03},
  journaltitle = {Program},
  volume       = {14},
  number       = {3},
  pages        = {130--137},
  issn         = {0033-0337, 0033-0337},
  doi          = {10.1108/eb046814},
  url          = {https://www.emerald.com/insight/content/doi/10.1108/eb046814/full/html},
  urldate      = {2021-07-19},
  langid       = {english}
}

@inproceedings{rabinovich2014,
  title      = {The {{Inverse Regression Topic Model}}},
  booktitle  = {International {{Conference}} on {{Machine Learning}}},
  author     = {Rabinovich, Maxim and Blei, David},
  date       = {2014-01-27},
  pages      = {199--207},
  publisher  = {{PMLR}},
  issn       = {1938-7228},
  url        = {http://proceedings.mlr.press/v32/rabinovich14.html},
  urldate    = {2021-07-30},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid     = {english}
}

@online{rao2021,
  title         = {{{Vec2GC}} -- {{A Graph Based Clustering Method}} for {{Text Representations}}},
  author        = {Rao, Rajesh N. and Chakraborty, Manojit},
  date          = {2021-04-15},
  eprint        = {2104.09439},
  eprinttype    = {arxiv},
  primaryclass  = {cs},
  url           = {http://arxiv.org/abs/2104.09439},
  urldate       = {2021-09-25},
  abstract      = {NLP pipelines with limited or no labeled data, rely on unsupervised methods for document processing. Unsupervised approaches typically depend on clustering of terms or documents. In this paper, we introduce a novel clustering algorithm, Vec2GC (Vector to Graph Communities), an end-to-end pipeline to cluster terms or documents for any given text corpus. Our method uses community detection on a weighted graph of the terms or documents, created using text representation learning. Vec2GC clustering algorithm is a density based approach, that supports hierarchical clustering as well.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Information Retrieval,Computer Science - Machine Learning}
}

@inproceedings{roder2015,
  title      = {Exploring the {{Space}} of {{Topic Coherence Measures}}},
  booktitle  = {Proceedings of the {{Eighth ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  author     = {Röder, Michael and Both, Andreas and Hinneburg, Alexander},
  date       = {2015-02-02},
  pages      = {399--408},
  publisher  = {{ACM}},
  location   = {{Shanghai China}},
  doi        = {10.1145/2684822.2685324},
  url        = {https://dl.acm.org/doi/10.1145/2684822.2685324},
  urldate    = {2021-10-01},
  abstract   = {Quantifying the coherence of a set of statements is a long standing problem with many potential applications that has attracted researchers from different sciences. The special case of measuring coherence of topics has been recently studied to remedy the problem that topic models give no guaranty on the interpretablity of their output. Several benchmark datasets were produced that record human judgements of the interpretability of topics. We are the first to propose a framework that allows to construct existing word based coherence measures as well as new ones by combining elementary components. We conduct a systematic search of the space of coherence measures using all publicly available topic relevance data for the evaluation. Our results show that new combinations of components outperform existing measures with respect to correlation to human ratings. Finally, we outline how our results can be transferred to further applications in the context of text mining, information retrieval and the world wide web.},
  eventtitle = {{{WSDM}} 2015: Eighth {{ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  isbn       = {978-1-4503-3317-7},
  langid     = {english}
}

@article{rosvall2008,
  title        = {Maps of Random Walks on Complex Networks Reveal Community Structure},
  author       = {Rosvall, M. and Bergstrom, C. T.},
  date         = {2008-01-29},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume       = {105},
  number       = {4},
  pages        = {1118--1123},
  issn         = {0027-8424, 1091-6490},
  doi          = {10.1073/pnas.0706851105},
  url          = {http://www.pnas.org/cgi/doi/10.1073/pnas.0706851105},
  urldate      = {2020-11-29},
  langid       = {english}
}

@article{rosvall2009,
  title        = {The Map Equation},
  author       = {Rosvall, M. and Axelsson, D. and Bergstrom, C. T.},
  date         = {2009-11},
  journaltitle = {Eur. Phys. J. Spec. Top.},
  volume       = {178},
  number       = {1},
  pages        = {13--23},
  issn         = {1951-6355, 1951-6401},
  doi          = {10.1140/epjst/e2010-01179-1},
  url          = {http://link.springer.com/10.1140/epjst/e2010-01179-1},
  urldate      = {2021-09-01},
  abstract     = {Many real-world networks are so large that we must simplify their structure before we can extract useful information about the systems they represent. As the tools for doing these simplifications proliferate within the network literature, researchers would benefit from some guidelines about which of the so-called community detection algorithms are most appropriate for the structures they are studying and the questions they are asking. Here we show that different methods highlight different aspects of a network’s structure and that the the sort of information that we seek to extract about the system must guide us in our decision. For example, many community detection algorithms, including the popular modularity maximization approach, infer module assignments from an underlying model of the network formation process. However, we are not always as interested in how a system’s network structure was formed, as we are in how a network’s extant structure influences the system’s behavior. To see how structure influences current behavior, we will recognize that links in a network induce movement across the network and result in system-wide interdependence. In doing so, we explicitly acknowledge that most networks carry flow. To highlight and simplify the network structure with respect to this flow, we use the map equation. We present an intuitive derivation of this flow-based and information-theoretic method and provide an interactive on-line application that anyone can use to explore the mechanics of the map equation. The differences between the map equation and the modularity maximization approach are not merely conceptual. Because the map equation attends to patterns of flow on the network and the modularity maximization approach does not, the two methods can yield dramatically different results for some network structures. To illustrate this and build our understanding of each method, we partition several sample networks. We also describe an algorithm and provide source code to efficiently decompose large weighted and directed networks based on the map equation.},
  langid       = {english}
}

@article{rosvall2010,
  title        = {Mapping {{Change}} in {{Large Networks}}},
  author       = {Rosvall, Martin and Bergstrom, Carl T.},
  editor       = {Rapallo, Fabio},
  date         = {2010-01-27},
  journaltitle = {PLoS ONE},
  volume       = {5},
  number       = {1},
  pages        = {e8694},
  issn         = {1932-6203},
  doi          = {10.1371/journal.pone.0008694},
  url          = {https://dx.plos.org/10.1371/journal.pone.0008694},
  urldate      = {2021-09-25},
  langid       = {english}
}

@article{salton1988,
  title        = {Term-Weighting Approaches in Automatic Text Retrieval},
  author       = {Salton, Gerard and Buckley, Christopher},
  date         = {1988-01},
  journaltitle = {Information Processing \& Management},
  volume       = {24},
  number       = {5},
  pages        = {513--523},
  issn         = {03064573},
  doi          = {10.1016/0306-4573(88)90021-0},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/0306457388900210},
  urldate      = {2021-07-19},
  langid       = {english}
}

@article{schofield2016,
  title        = {Comparing {{Apples}} to {{Apple}}: The {{Effects}} of {{Stemmers}} on {{Topic Models}}},
  shorttitle   = {Comparing {{Apples}} to {{Apple}}},
  author       = {Schofield, Alexandra and Mimno, David},
  date         = {2016-12},
  journaltitle = {TACL},
  volume       = {4},
  pages        = {287--300},
  issn         = {2307-387X},
  doi          = {10.1162/tacl_a_00099},
  url          = {https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00099},
  urldate      = {2020-11-06},
  abstract     = {Rule-based stemmers such as the Porter stemmer are frequently used to preprocess English corpora for topic modeling. In this work, we train and evaluate topic models on a variety of corpora using several different stemming algorithms. We examine several different quantitative measures of the resulting models, including likelihood, coherence, model stability, and entropy. Despite their frequent use in topic modeling, we find that stemmers produce no meaningful improvement in likelihood and coherence and in fact can degrade topic stability.},
  langid       = {english}
}

@inproceedings{schofield2017,
  title      = {Pulling {{Out}} the {{Stops}}: Rethinking {{Stopword Removal}} for {{Topic Models}}},
  shorttitle = {Pulling {{Out}} the {{Stops}}},
  booktitle  = {Proceedings of the 15th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}: Volume 2, {{Short Papers}}},
  author     = {Schofield, Alexandra and Magnusson, Måns and Mimno, David},
  date       = {2017-04},
  pages      = {432--436},
  publisher  = {{Association for Computational Linguistics}},
  location   = {{Valencia, Spain}},
  url        = {https://aclanthology.org/E17-2069},
  urldate    = {2021-09-27},
  abstract   = {It is often assumed that topic models benefit from the use of a manually curated stopword list. Constructing this list is time-consuming and often subject to user judgments about what kinds of words are important to the model and the application. Although stopword removal clearly affects which word types appear as most probable terms in topics, we argue that this improvement is superficial, and that topic inference benefits little from the practice of removing stopwords beyond very frequent terms. Removing corpus-specific stopwords after model inference is more transparent and produces similar results to removing those words prior to inference.},
  eventtitle = {{{EACL}} 2017}
}

@online{shiNewEvaluationFramework2019,
  title         = {A New Evaluation Framework for Topic Modeling Algorithms Based on Synthetic Corpora},
  author        = {Shi, Hanyu and Gerlach, Martin and Diersen, Isabel and Downey, Doug and Amaral, Luis A. N.},
  date          = {2019-01-28},
  eprint        = {1901.09848},
  eprinttype    = {arxiv},
  primaryclass  = {physics},
  url           = {http://arxiv.org/abs/1901.09848},
  urldate       = {2020-02-27},
  abstract      = {Topic models are in widespread use in natural language processing and beyond. Here, we propose a new framework for the evaluation of probabilistic topic modeling algorithms based on synthetic corpora containing an unambiguously defined ground truth topic structure. The major innovation of our approach is the ability to quantify the agreement between the planted and inferred topic structures by comparing the assigned topic labels at the level of the tokens. In experiments, our approach yields novel insights about the relative strengths of topic models as corpus characteristics vary, and the first evidence of an “undetectable phase” for topic models when the planted structure is weak. We also establish the practical relevance of the insights gained for synthetic corpora by predicting the performance of topic modeling algorithms in classification tasks in real-world corpora.},
  archiveprefix = {arXiv},
  langid        = {english},
  keywords      = {_tablet,⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,Physics - Physics and Society},
  annotation    = {ZSCC: 0000000}
}

@article{suominenMapScienceTopic2016,
  title        = {Map of Science with Topic Modeling: Comparison of Unsupervised Learning and Human-Assigned Subject Classification},
  shorttitle   = {Map of Science with Topic Modeling},
  author       = {Suominen, Arho and Toivanen, Hannes},
  date         = {2016-10},
  journaltitle = {J Assn Inf Sci Tec},
  volume       = {67},
  number       = {10},
  pages        = {2464--2476},
  issn         = {23301635},
  doi          = {10/f8549r},
  url          = {http://doi.wiley.com/10.1002/asi.23596},
  urldate      = {2020-04-30},
  issue        = {10},
  langid       = {english},
  keywords     = {_tablet},
  annotation   = {ZSCC: 0000000}
}

@inproceedings{teh2005,
  title      = {Sharing {{Clusters}} among {{Related Groups}}: Hierarchical {{Dirichlet Processes}}},
  shorttitle = {Sharing {{Clusters}} among {{Related Groups}}},
  booktitle  = {Advances in {{Neural Information Processing Systems}}},
  author     = {Teh, Yee and Jordan, Michael and Beal, Matthew and Blei, David},
  date       = {2005},
  volume     = {17},
  publisher  = {{MIT Press}},
  url        = {https://papers.nips.cc/paper/2004/hash/fb4ab556bc42d6f0ee0f9e24ec4d1af0-Abstract.html},
  urldate    = {2021-09-24}
}

@article{vinh2010,
  title        = {Information {{Theoretic Measures}} for {{Clusterings Comparison}}: Variants, {{Properties}}, {{Normalization}} and {{Correction}} for {{Chance}}},
  shorttitle   = {Information {{Theoretic Measures}} for {{Clusterings Comparison}}},
  author       = {Vinh, Nguyen Xuan and Epps, Julien and Bailey, James},
  date         = {2010-12-01},
  journaltitle = {J. Mach. Learn. Res.},
  volume       = {11},
  pages        = {2837--2854},
  issn         = {1532-4435},
  abstract     = {Information theoretic measures form a fundamental class of measures for comparing clusterings, and have recently received increasing interest. Nevertheless, a number of questions concerning their properties and inter-relationships remain unresolved. In this paper, we perform an organized study of information theoretic measures for clustering comparison, including several existing popular measures in the literature, as well as some newly proposed ones. We discuss and prove their important properties, such as the metric property and the normalization property. We then highlight to the clustering community the importance of correcting information theoretic measures for chance, especially when the data size is small compared to the number of clusters present therein. Of the available information theoretic based measures, we advocate the normalized information distance (NID) as a general measure of choice, for it possesses concurrently several important properties, such as being both a metric and a normalized measure, admitting an exact analytical adjusted-for-chance form, and using the nominal [0,1] range better than other normalized variants.}
}

@article{wallace2009,
  title        = {A New Approach for Detecting Scientific Specialties from Raw Cocitation Networks},
  author       = {Wallace, Matthew L. and Gingras, Yves and Duhon, Russell},
  date         = {2009-02},
  journaltitle = {J. Am. Soc. Inf. Sci.},
  volume       = {60},
  number       = {2},
  pages        = {240--246},
  issn         = {15322882, 15322890},
  doi          = {10/djq259},
  url          = {http://doi.wiley.com/10.1002/asi.20987},
  urldate      = {2020-05-10},
  abstract     = {We use a technique recently developed by Blondel et al. (2008) in order to detect scientific specialties from author cocitation networks. This algorithm has distinct advantages over most of the previous methods used to obtain cocitation “clusters”, since it avoids the use of similarity measures, relies entirely on the topology of the weighted network and can be applied to relatively large networks. Most importantly, it requires no subjective interpretation of the cocitation data or of the communities found. Using two examples, we show that the resulting specialties are the smallest coherent “group” of researchers (within a hierarchy of cluster sizes) and can thus be identified unambiguously. Furthermore, we confirm that these communities are indeed representative of what we know about the structure of a given scientific discipline and that, as specialties, they can be accurately characterized by a few keywords (from the publication titles). We argue that this robust and efficient algorithm is particularly well-suited to cocitation networks, and that the results generated can be of great use to researchers studying various facets of the structure and evolution of science.},
  issue        = {2},
  langid       = {english},
  keywords     = {_tablet},
  annotation   = {ZSCC: 0000000}
}

@inproceedings{wallach2009,
  title      = {Rethinking {{LDA}}: Why {{Priors Matter}}},
  shorttitle = {Rethinking {{LDA}}},
  booktitle  = {Advances in {{Neural Information Processing Systems}}},
  author     = {Wallach, Hanna and Mimno, David and McCallum, Andrew},
  date       = {2009},
  volume     = {22},
  publisher  = {{Curran Associates, Inc.}},
  url        = {https://papers.nips.cc/paper/2009/hash/0d0871f0806eae32d30983b62252da50-Abstract.html},
  urldate    = {2021-09-24}
}

@inproceedings{wang2011,
  title      = {Online {{Variational Inference}} for the {{Hierarchical Dirichlet Process}}},
  booktitle  = {Proceedings of the {{Fourteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author     = {Wang, Chong and Paisley, John and Blei, David},
  date       = {2011-06-14},
  pages      = {752--760},
  publisher  = {{JMLR Workshop and Conference Proceedings}},
  issn       = {1938-7228},
  url        = {https://proceedings.mlr.press/v15/wang11a.html},
  urldate    = {2021-09-24},
  abstract   = {The hierarchical Dirichlet process (HDP) is a Bayesian nonparametric model that can be used to model mixed-membership data with a potentially infinite number of components.  It has been applied widely in probabilistic topic modeling, where the data are documents and the components are distributions of terms that reflect recurring patterns (or “topics”) in the collection.  Given a document collection, posterior inference is used to determine the number of topics needed and to characterize their distributions.  One limitation of HDP analysis is that existing posterior inference algorithms require multiple passes through all the data—these algorithms are intractable for very large scale applications.  We propose an online variational inference algorithm for the HDP, an algorithm that is easily applicable to massive and streaming data.  Our algorithm is significantly faster than traditional inference algorithms for the HDP, and lets us analyze much larger data sets.  We illustrate the approach on two large collections of text, showing improved performance over online LDA, the finite counterpart to the HDP topic model. [pdf]},
  eventtitle = {Proceedings of the {{Fourteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid     = {english}
}

@inproceedings{zeng2018,
  title     = {A {{Distributed Infomap Algorithm}} for {{Scalable}} and {{High}}-{{Quality Community Detection}}},
  booktitle = {Proceedings of the 47th {{International Conference}} on {{Parallel Processing}}},
  author    = {Zeng, Jianping and Yu, Hongfeng},
  date      = {2018-08-13},
  series    = {{{ICPP}} 2018},
  pages     = {1--11},
  publisher = {{Association for Computing Machinery}},
  location  = {{New York, NY, USA}},
  doi       = {10.1145/3225058.3225137},
  url       = {https://doi.org/10.1145/3225058.3225137},
  urldate   = {2021-09-24},
  abstract  = {Community detection is essential to various graph analysis applications. Infomap is a graph clustering algorithm capable of achieving high-quality communities. However, it remains a very challenging problem to effectively apply Infomap on large graphs. By analyzing communication and workload patterns of Infomap and leveraging a distributed delegate partitioning and distribution method, we develop a new heuristic strategy to carefully coordinate the community constitution from the vertices of a graph in a distributed environment, and achieve the convergence of the distributed clustering algorithm. We have implemented our optimized algorithm using MPI (Message Passing Interface), which can be easily employed or extended to massively distributed computing systems. We analyze the correctness of our algorithm, and conduct an intensive experimental study to investigate the communication and computation cost of our distributed algorithm, which has not shown in previous work. The results demonstrate the scalability and the correctness of our distributed Infomap algorithm with large-scale real-world datasets.},
  isbn      = {978-1-4503-6510-9},
  keywords  = {accuracy,Community detection,Infomap,large graphs,scalability}
}

@book{zipf1965,
  title      = {The {{Psycho}}-{{Biology}} of {{Language}}: An {{Introdution}} to {{Dynamic Philology}}},
  shorttitle = {The {{Psycho}}-{{Biology}} of {{Language}}},
  author     = {Zipf, George K.},
  date       = {1965-09-15},
  publisher  = {{MIT Press}},
  location   = {{Cambridge, MA, USA}},
  abstract   = {An investigation of speech as a form of behavior, examined in the manner of the exact sciences by the direct application of statistical principles to the objective speech-phenomena.},
  isbn       = {978-0-262-24005-5},
  langid     = {english},
  pagetotal  = {336}
}


